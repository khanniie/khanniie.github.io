{"componentChunkName":"component---src-templates-page-js","path":"/nasa","result":{"data":{"markdownRemark":{"frontmatter":{"title":"NASA JPL: Pixelise+","date":"Fall 2020","path":"/nasa","author":"Elliot","excerpt":"Working with NASA JPL, we developed an interactive tool that enables scientists to compare data from their PIXL tool.","tags":["Frontend","Data vis","React","d3.js","ML"],"coverImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAECBAX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHm5UyCg//EABoQAAEFAQAAAAAAAAAAAAAAAAABAhAhMUH/2gAIAQEAAQUCUdtnI//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAQADAAAAAAAAAAAAAAAAAAEAIDH/2gAIAQEABj8C0jX/xAAZEAADAQEBAAAAAAAAAAAAAAAAAREhEDH/2gAIAQEAAT8hZRhWXlIHqcZ//9oADAMBAAIAAwAAABBDD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQACAgMAAAAAAAAAAAAAAAEAESFBMXGR/9oACAEBAAE/EOfGCgc3E2bZVTqdnsSlbbSAOoQZ/9k=","aspectRatio":1.7857142857142858,"src":"/static/680f9b1c05e619d9088e30cb02b1f869/14b42/nasa.jpg","srcSet":"/static/680f9b1c05e619d9088e30cb02b1f869/f836f/nasa.jpg 200w,\n/static/680f9b1c05e619d9088e30cb02b1f869/2244e/nasa.jpg 400w,\n/static/680f9b1c05e619d9088e30cb02b1f869/14b42/nasa.jpg 800w,\n/static/680f9b1c05e619d9088e30cb02b1f869/47498/nasa.jpg 1200w,\n/static/680f9b1c05e619d9088e30cb02b1f869/b24a9/nasa.jpg 1560w","sizes":"(max-width: 800px) 100vw, 800px"}}},"coverVideo":{"publicURL":"/static/54dab911696b372e00c6b54012d42b0b/nasa.mp4"},"links":""},"id":"5720aede-bfed-5d8a-aa99-5a265b75d988","html":"<p class=\"para\"><strong>Working with NASA JPL, we developed an interactive tool that enables scientists to compare data from their PIXL tool.</strong> This project was a collaboration between with Lukas Hermann, Nur Yildirim, and Shravya Bhat. </p>\n<h1>üåê <a href=\"https://cmu-ids-2020.github.io/fp-airbnb-rats/\">Live URL</a></h1>\n<h1>‚å®Ô∏è <a href=\"https://github.com/CMU-IDS-2020/fp-airbnb-rats\">Github Repository</a></h1>\n<h3>Clustering server rate limitations</h3>\n<p class=\"para\">We are running the clustering backend on an AWS instance. Because it's a free trail of an instance, it will expire around Janurary 9th, 2021. The app will continue to work afterwards, but the clustering feature will not work unless you set up your own server. We are also using a free proxy server service (CORS-anywhere) to forward our CORS requests because otherwise we will get https client to http errors. <strong>Due to the limits of free AWS and the free proxy server service, the rate and speed of responses may be limited if the app traffic is very high.</strong></p>\n<p class=\"para\">In this case there are two possible solutions:</p>\n<ol>\n<li>Contact me and I can set up an alternative proxy server for you</li>\n<li>Clone our repository and set it up locally using the instructions in the README.</li>\n</ol>\n<p class=\"para\"><img src=\"../images/nasa/pixl.gif\" alt=\"demo gif\"></p>\n<p class=\"para\">During the project, I maintained and set up the components in our web app, which was built in React. I implemented the design of the interface, set up information coordination between components, and programmed the majority of the interactions in the interface, such as: </p>\n<ul>\n<li>the lasso tool</li>\n<li>adding and removing groups</li>\n<li>the image viewer/overlay component with pan/zoom</li>\n<li>group annotation and locking</li>\n<li>element filtering and sorting</li>\n<li>and more! </li>\n</ul>\n<p class=\"para\">I also worked with Lukas on the visualizations using d3.js. Lastly, I conceptualized and designed the core interactions, which I mocked up in Figma and validated with the key stakeholder, a NASA Co-Investigator, with a user study protocol that I created.</p>\n<h1>Abstract</h1>\n<p class=\"para\">NASA JPL scientists working on the micro x-ray fluorescence (microXRF) spectroscopy data collected from Mars surface perform data analysis to look for signs of past microbial life on Mars. Their data analysis workflow mainly involves identifying mineral compounds through the element abundance in spatially distributed data points. Working with the NASA JPL team, we identified pain points and needs to further develop their existing data visualization and analysis tool. Specially, the team desired improvements for the process of creating and interpreting mineral composition groups. To address this problem, we developed an interactive tool that enables scientists to (1) semi-automatically cluster the data, and (2) compare the clusters and individual data points to make informed decisions about mineral compositions. Our tool supports a hybrid data analysis workflow where the user can manually refine the machine generated clusters. </p>\n<h1>Introduction</h1>\n<p class=\"para\">Our project is carried out in collaboration with NASA Jet Propulsion Laboratory Human Interfaces Group in order to support the exploratory analysis of astrobiological data collected via Mars 2020 Perseverance rover. The mission of NASA scientists is to look for signs of past microbial life on Mars by examining the chemical makeup of rock and soil textures at a very fine scale. This is performed by analysing the micro x-ray fluorescence (microXRF) spectroscopy data collected by The Planetary Instrument for X-ray Lithochemistry (PIXL). By looking at the concentration and the spatial distribution of elements, the scientists are able to interpret the microXRF data and identify the mineral compounds that were likely created by microbes [2].</p>\n<p class=\"para\">The PIXL instrument team performing this work include astrobiologists, sedimentologists, igneous petrologists, spectroscopists, and geochemists. An initial data visualisation tool prototype, Pixlise, is already in place to support scientists in this process [3]. Our preliminary discussions with the project team, therefore, focused on identifying the tasks and information needs of the scientists that might not be supported by the current app. In Pixlise, the data analysis tasks are mainly carried out using a map interface as the collected data involves spatially defined points with coordinates. The data analysis workflow has two major stages: (1) assigning elements to data points based on the spectroscopy data, (2) creating and interpreting mineral composition groups based on elemental abundance. Through a collaborative problem defining process, we decided to focus on the latter stage as the system in place is mostly targeted towards the former stage tasks.</p>\n<p class=\"para\">In order to support the process of interpretation, we developed an interactive data visualization tool that <strong>presents contributions in terms of the comparison and clustering of data</strong>. Our standalone app presents an exploration of features and improvements that could be embedded into the future versions of the Pixlise tool. In the following sections, we first describe the existing tool and dataset, we then describe our tool development process and the interactive visualization techniques we propose. Finally, we walk through some use cases to illustrate the scenarios where our tool can support the data analysis process.</p>\n<p class=\"para\">This application was developed over the course of about a month. We first had an initial meeting with one of our stakeholders, the project lead UX manager at NASA JPL where we gathered resources and learned about the possible problems that we could tackle. After more research into the datasets provided and some brainstorming about possible problem scopes, we presented initial mockups to the UX manager. Based on the feedback, we produced new mockups of 10 possible new interaction models or features and presented them to the co-investigator astrologists, who is a main beneficiary. Their input helped to validate our initial mockups, guide future design decisions and also helped us to prioritize which features to focus on. In the remaining weeks, we built the final application based on these findings.</p>\n<h1>Process mockups</h1>\n<p class=\"para\"><img src=\"../images/nasa/process1.png\" alt=\"process1\">\n<em>Initial sketches</em></p>\n<p class=\"para\"><img src=\"../images/nasa/process2.png\" alt=\"process2\">\n<em>First pass at showing comparison</em></p>\n<p class=\"para\"><img src=\"../images/nasa/process3.png\" alt=\"process3\">\n<em>10 possible interactions</em></p>\n<h1>Dataset, Pixlise App and The Data Analysis Workflow</h1>\n<p class=\"para\">PIXL data set is a collection of spatially localized spectroscopy data as the instrument on Perseverance rover passes over the sampling area. The microXRF data has 1024 ordinal channels, each with a count of x-rays sensed by the instrument at that particular channel. The dataset is organized into a CSV file that contains information on points, coordinate data (X, Y, Z), and spectrum data with more than 4000 points per each data point.</p>\n<p class=\"para\">The current data visualization tool facilitates the data analysis workflow through multiple interaction techniques (Figure 1). The main interaction touchpoint is the context image pane (Figure 1a) that displays the discrete points in the microXRF data. Using this map display allows scientists to visually isolate, select, and analyze discrete geological features within a sample data point [1]. As a first step, users have to assign elements to each data point. The spectrum pane (Figure 1b) visualizes the energy spectral density [5] of data points individually or in bulk. By looking at the peaks in the wavelength, users are able to infer element transitions. The instrument can detect over 20 elements, including Na, Mg, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Br, Rb, Sr, Y, Ga, Ge, As, Zr, Rb, Sr, Y and Zr at 10‚Äôs ppm level [2]. The element assignment process is semi-automatic as the tool can infer the presence of some elements, yet the domain knowledge of scientists are necessary to fine tune and validate the assignment.</p>\n<p class=\"para\"><img src=\"../images/nasa/report_img_2.png\" alt=\"report image 2\"></p>\n<p class=\"para\"><em>Figure 1: Pixlise Interface</em></p>\n<p class=\"para\">Once the elements are assigned, users introspect the element abundance to make associations for mineral identification using supporting panes. The histogram tool (Figure 1c) shows the quantified weight percentages of different elements in selected samples in linear or log scale. The chord diagram (Figure 1d), the ternary diagram (Figure 1e), and the binary diagram (Figure 1f) indicate the correlations between abundant elements to inform the interpretation of mineral compositions.</p>\n<p class=\"para\">While the Pixlise tool supports the process of element assignment and quantification, there is little support in terms of comparison between points and regions. Our interviews with the stakeholders, including the project lead UX Manager and the co-lead research scientist, revealed that there are several pain points and needs in terms of selection, grouping, annotation, clustering, and comparison. Specifically, the current interface does not provide any means to do multiple selections (individual or group) at the same time in order to compare the element distributions across different regions. Additionally, the scientists desired to have auto generated clusters with various algorithm options, so that they do not have to start from scratch and instead semi-automatically create groups of the mapped area. Our tool aims to address these gaps through extending the existing data analysis workflow with novel and targeted interaction design solutions.</p>\n<h1>Pixlise+</h1>\n<p class=\"para\">We present an interactive data visualization tool, Pixlise+, that extends the current system in place. We built our tool as a standalone application instead of integrated development, as the current architecture has several authentication layers that makes the development process difficult. However, our tool builds on the current application in terms of interface layout, interaction styles, and overall design to maintain familiarity.</p>\n<p class=\"para\"><img src=\"../images/nasa/report_img_1.jpg\" alt=\"report image 1\"></p>\n<p class=\"para\"><em>Figure 2: Pixlise+ Interface</em></p>\n<p class=\"para\">Pixlise+ interface consists of four main components: Context Image (Figure 2a), Comparison Sandbox (Figure 2b), Clustering (Figure 2c), and Parallel Coordinate Plots (Figure 2d). Below, we describe each component in detail.</p>\n<p class=\"para\"><strong>Context Image:</strong> This 2D map is the main interaction touchpoint for the application. The visualization plots the collected data on the geographical context image as dot per point. There are 6400 data points arranged into a grid based on their spatial coordinates. Overlaying the datapoint over the real terrain view allows scientists to select and analyze data in accordance with the context. There are three major actions the users can take in this view. First, using the navigation icon, they can pan and zoom (in or out). Second, using the examining tool (eye icon), they can hover over individual points to view related data in isolation, which is displayed in the Comparison Sandbox. Finally, the scientists can select and group the data points using the lasso tool and component dropdown menu. Once a component group is created by clicking on ‚Äúadd item‚Äù, the selected data points will be displayed in that group‚Äôs color. Consequent lasso actions will add to the selection, while holding down shift will remove points from the selection. Detailed information about the groups in terms of element concentration is displayed in the Comparison Sandbox, so that the scientists can iterate on grouping data into clusters based on similarity. </p>\n<p class=\"para\"><strong>Comparison Sandbox:</strong> This view displays the element percentages in the selected data points as histograms. As opposed to displaying one histogram at a time as in the previous implementation, our tool can display up to 20 histograms for the comparison of potential mineral compositions across clusters. When initiated the Comparison Sandbox displays the average element weights of all samples in the dataset in logarithmic or linear scale. The element symbols are displayed as columns at the top and users are able to hide less important elements to focus on the most relevant ones. The histogram can be sorted based on the element abundance (using means, high to low or low to high) or the coefficient of variation (standard deviation divided by the mean). The histogram tooltip displays more detailed statistical information per element, including mean, standard deviation, min, max, Q1, Q3, and the median. This information is useful to help the user determine how the distribution of data looks. Min, max, Q1, Q3 and median are especially important if the distribution is not a normal distribution. Additionally, variance in an element is shown as an inner bar within each bar of the histogram, which informs whether selected data points demonstrate similar characteristics.</p>\n<p class=\"para\">As users create multiple groups using the selection and component creation tools in the Context Image view, the histograms in the Comparison Sandbox update to display multiple groups at once. Using histogram controls, users can lock and unlock a group before finalizing their selection. Additionally, they can annotate the group with insights. Once a group is annotated, hovering over the group in the Context Image view shows the annotations overlaid on top. Finally, when users want to examine how a single data point fits in a group, hovering over the data point in the Context Image view displays the element percentages of the individual data point across all histograms as overlaid ticks on bars.</p>\n<p class=\"para\"><strong>Parallel Coordinates Plots:</strong> Histograms are useful in characterizing groups through element abundance, but it can be difficult to distinguish fine grain differences among the data. To enable a more granular exploration, Parallel Coordinates Plots (PCP) displays percent values based on the minimum and maximum range. Therefore, nuanced differences between groups can be seen per each element.</p>\n<p class=\"para\"><strong>Clustering:</strong> Grouping the data points into clusters are usually carried out manually as the scientists have extensive expertise in terms of identifying geologic features in images. However, machine learning algorithms can provide particular or generalizable insights depending on the dataset. Our tool utilizes various ML techniques for auto generating clusters as starting points for a semi-automatic data analysis workflow. Clustering tool provides three different algorithms: k-means clustering, hierarchical clustering, and minimum maximum. K-means clustering partitions the dataset into a user defined number of clusters using the distance between the points. Scientists can choose to apply dimensionality reduction using PCA or t-SNE. PCA performs deterministic linear dimensionality reduction and takes in variance as a hyperparameter. T-SNE, on the other hand, performs probabilistic and non-linear dimensionality reduction and takes in perplexity as a hyperparameter.</p>\n<p class=\"para\">Hierarchical clustering builds a hierarchy of clusters based on a distance metric defined by the linkage hyperparameters (ward, complete, average, or single). Similar to k-means, the number of clusters is user defined in hierarchical clustering. Finally, users can create auto generated groups without using a clustering algorithm. In this case, they can group the dataset based on maximum or minimum abundance of elements. The clustering tool aims to enable scientists to play around with different clustering options and fine tune parameters for finding insights.</p>\n<h1>Technical Implementation</h1>\n<p class=\"para\">We developed our application using React and deployed it using Github pages. Our code is open source [4]. While React was used to develop the UI and coordinate the different components across the application, D3.js, a popular visualization Javascript library, was used to create the visualizations within the components. </p>\n<p class=\"para\">For the clustering implementation, we use a Python backend to dynamically generate cluster information based on the many different hyperparameters that our app allows.The client app and the Python backend communicate using Flask and an ajax call using jQuery. The backend itself is hosted on Amazon Web Services. The clustering backend uses popular data science libraries such as sklearn and pandas. Sklearn is used to perform all of our machine learning algorithms, including hierarchical clustering, t-SNE, PCA, and k-means. Our application also enables hyperparameters such as linkage, variance percentage, perplexity, and number of clusters, to be passed to the backend as well.</p>\n<h1>Results</h1>\n<p class=\"para\">We present a walkthrough of our tool in our project demonstration video using a sample dataset called ‚ÄúKing‚Äôs Court‚Äù. This dataset has samples from a terrain with a crater-like geological feature. Using the clustering tool, we were able to test various algorithms and hyperparameters for grouping the dataset into meaningful subsets. Our explorative results show that both PCA and t-SNE were able to identify the large cluster around the crater (Figure 2a-1) with multiple discrete groups around its perimeters (Figure 2a-2). Once the initial groups were identified using the clustering algorithm, we used the hovering feature in the Context Image tools for examining individual data points within clusters. Based on the histogram information in the Comparison Sandbox, we were able to add or remove specific data points using the lasso tool.</p>\n<h1>Discussion and Future Work</h1>\n<p class=\"para\">The main contributions of our work include interaction techniques for the comparative analysis and clustering of microXRF spectroscopy data in the context of the PIXL project. Our tool Pixlise+ enabled a semi-automatic data analysis workflow that enabled an augmented grouping of microXRF data based on element abundance. Using built-in statistical analyses, scientists can compare several clusters at once to fine tune and validate the grouping of the data. Moreover, tools that support documentation including locking and annotation open the way to future developments for multi-user collaboration.</p>\n<p class=\"para\">Our work reveals several directions for future research. For the overall data analysis workflow, blending human and machine intelligence raises major research questions. How can we combine manually created groups with machine generated clusters to provide support scientists‚Äô agency while removing tedious work? What kind of interaction techniques (e.g. overlays) might be suited for representing human and machine generated groups? Should the system suggest potential clusters to validate human insights through mixed initiative interaction? These are some questions we would like to explore going forward. In addition, sharing annotated datasets present an interesting direction for future research in terms of collaborative and asynchronous workflows. Potential techniques include exporting screenshots, sharing workspaces, and sharing the dataset to be opened with a different program or tool. We will also explore the database requirements to allow saving and sharing.</p>\n<p class=\"para\">Finally, our short term goals include the integration of Pixlise+ into the current system used by NASA JPL. We will continue working with our collaborators to collect feedback on our tool through user testing, iterative development, and deployment.</p>\n<h1>Special thanks</h1>\n<p class=\"para\">Thank you to IDS professors Dominic Mortiz and Adam Perer and the TAs for their help! And thank you to the researchers and designers at NASA for letting us work with them on this project!</p>\n<p class=\"para\"><strong>References</strong></p>\n<p class=\"para\">[1] PIXELATE: novel visualization and computational methods for the analysis of astrobiological spectroscopy data. <a href=\"https://www.researchgate.net/publication/338924853_PIXELATE_novel_visualization_and_computational_methods_for_the_analysis_of_astrobiological_spectroscopy_data\">link</a></p>\n<p class=\"para\">[2] PIXL for Scientists. <a href=\"https://mars.nasa.gov/mars2020/spacecraft/instruments/pixl/for-scientists/\">link</a></p>\n<p class=\"para\">[3] PIXLISE Mars Rock Sample Investigation. <a href=\"https://datavis.caltech.edu/projects/pixl/\">link</a></p>\n<p class=\"para\">[4] Pixlise+ Github Repository. <a href=\"https://github.com/CMU-IDS-2020/fp-airbnb-rats\">link</a></p>\n<p class=\"para\">[5] Spectral Density. <a href=\"https://en.wikipedia.org/wiki/Spectral_density\">link</a></p>","excerpt":"Working with NASA JPL, we developed an interactive tool that enables scientists to compare data from their PIXL tool. This project was a‚Ä¶"}},"pageContext":{"type":"posts","next":{"id":"5d3433e6-2945-58dc-97a9-725b0bbf7073","children":[],"parent":"f5f5b024-afb4-558a-9cb5-54bcf0d9a3b4","internal":{"content":"# About\nMldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.\nMldraw is a web app that uses a layered vector drawing system where each layer can be given a different machine learning model that translates user input. The user will give us a line drawing of edges, and our app's backend server renders the translation using whatever model is assigned to that layer.\nFurthermore, we've also made it easy to import custom ML models for researchers so that they can use our tool to experiment with their models!\nMade in collaboration with [Aman Tiwari](http://www.aman.work/)!\nI designed and built the UI using HTML/CSS/Typescript, and I also worked on the drawing and layer functionality, such as the\nvector drawing, drag and dropping objects, layer building, etc.\nMldraw‚Äôs interface is inspired by cute, techy/anti-techy retro aesthetics, such as the work of Sailor Mercury and the Bubblesort Zines.  We wanted it to be fun, novel, exciting and differentiated from the world of arxiv papers.\nMldraw was born out of seeing the potential of the body of research done using pix2pix to turn drawings into other images and the severe lack of a usable, ‚Äúuseful‚Äù and accessible tool to utilize this technology.\n\n# Videos\n\n<iframe class=\"iframe-560h\" src=\"https://player.vimeo.com/video/321893512\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<iframe class=\"iframe-620h\" src=\"https://player.vimeo.com/video/378759111\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Still in progress\nWe have a great demo, but have plans to bug-test, add more exciting ML models and features, and deploy at mldraw.com. We're also still experimenting and making big changes to the UI / UX! More documentation coming soon.\n![edges2pikachu example](../images/mldraw/mldraw_beforeafter.jpg) *Left: User line drawing, Right: rendered version. Combined pikachu model and handbag model*\n\n# Implementation\nMldraw is implemented as a Typescript frontend using choo.js as a UI framework, with a Python registry server and a Python adapter library, along with a number of instantiations of the adapter library for specific models.\nThe frontend communicates with the registry server using socket.io, which then passes to the frontend a list of models and their URLs. The frontend then communicates directly to the models.  This enables us e.g. to host a registry server for Mldraw without having to pay the cost of hosting every model it supports.\nMldraw also supports models that run locally on the client (in the above video, the cat, Pikachu and bag models run locally, while the other models are hosted on remote servers).\nIn service of the above desire to make Mldraw extensible, we have made it easy to add a new model ‚Äì all that is required is some Python interface to the model, and to define a function that takes in an image and returns an image. Our model adapter will handle the rest of it, including registering the model with the server hosting a Mldraw interface.\n\n# Progress Documentation\n\n<video autoplay loop muted poster=\"../images/mldraw/mldraw_sequence.jpg\">\n    <source src=\"../images/mldraw/mldraw_sequence.mp4\">\n</video>\n\n## CSS animations\n![render button interaction gif](../images/mldraw/button-interactions.gif)*Examples of small css animated interactions, Left: tooltip + pop animation on hover + lingering expansion until unhover, Middle: render button disappears when canvas in use, fades back in when done, Right: render button spins while rendering*\n\n\n\n![toolbar interaction gif](../images/mldraw/toolbar.gif)*toolbar animations: nod when usable, shake when unusable for that model*\n","type":"MarkdownRemark","contentDigest":"edbda9c74ea9fda243d188ff8ce43228","counter":159,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"mldraw Drawing Tool","path":"/ml-draw","date":"Spring 2019 - Spring 2020","coverImage":"../images/mldraw_thumb.jpg","coverVideo":"../assets/mldraw.mp4","excerpt":"Mldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.","tags":["Frontend","web","Typescript","ML","UI design"],"links":"","readMoreButton":"See documentation","order":3,"shortTitle":"Mldraw"},"excerpt":"","rawMarkdownBody":"# About\nMldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.\nMldraw is a web app that uses a layered vector drawing system where each layer can be given a different machine learning model that translates user input. The user will give us a line drawing of edges, and our app's backend server renders the translation using whatever model is assigned to that layer.\nFurthermore, we've also made it easy to import custom ML models for researchers so that they can use our tool to experiment with their models!\nMade in collaboration with [Aman Tiwari](http://www.aman.work/)!\nI designed and built the UI using HTML/CSS/Typescript, and I also worked on the drawing and layer functionality, such as the\nvector drawing, drag and dropping objects, layer building, etc.\nMldraw‚Äôs interface is inspired by cute, techy/anti-techy retro aesthetics, such as the work of Sailor Mercury and the Bubblesort Zines.  We wanted it to be fun, novel, exciting and differentiated from the world of arxiv papers.\nMldraw was born out of seeing the potential of the body of research done using pix2pix to turn drawings into other images and the severe lack of a usable, ‚Äúuseful‚Äù and accessible tool to utilize this technology.\n\n# Videos\n\n<iframe class=\"iframe-560h\" src=\"https://player.vimeo.com/video/321893512\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<iframe class=\"iframe-620h\" src=\"https://player.vimeo.com/video/378759111\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Still in progress\nWe have a great demo, but have plans to bug-test, add more exciting ML models and features, and deploy at mldraw.com. We're also still experimenting and making big changes to the UI / UX! More documentation coming soon.\n![edges2pikachu example](../images/mldraw/mldraw_beforeafter.jpg) *Left: User line drawing, Right: rendered version. Combined pikachu model and handbag model*\n\n# Implementation\nMldraw is implemented as a Typescript frontend using choo.js as a UI framework, with a Python registry server and a Python adapter library, along with a number of instantiations of the adapter library for specific models.\nThe frontend communicates with the registry server using socket.io, which then passes to the frontend a list of models and their URLs. The frontend then communicates directly to the models.  This enables us e.g. to host a registry server for Mldraw without having to pay the cost of hosting every model it supports.\nMldraw also supports models that run locally on the client (in the above video, the cat, Pikachu and bag models run locally, while the other models are hosted on remote servers).\nIn service of the above desire to make Mldraw extensible, we have made it easy to add a new model ‚Äì all that is required is some Python interface to the model, and to define a function that takes in an image and returns an image. Our model adapter will handle the rest of it, including registering the model with the server hosting a Mldraw interface.\n\n# Progress Documentation\n\n<video autoplay loop muted poster=\"../images/mldraw/mldraw_sequence.jpg\">\n    <source src=\"../images/mldraw/mldraw_sequence.mp4\">\n</video>\n\n## CSS animations\n![render button interaction gif](../images/mldraw/button-interactions.gif)*Examples of small css animated interactions, Left: tooltip + pop animation on hover + lingering expansion until unhover, Middle: render button disappears when canvas in use, fades back in when done, Right: render button spins while rendering*\n\n\n\n![toolbar interaction gif](../images/mldraw/toolbar.gif)*toolbar animations: nod when usable, shake when unusable for that model*\n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/mldraw.md","__gatsby_resolved":{"frontmatter":{"path":"/ml-draw"}}},"previous":{"id":"3d8b1d1f-bc63-52ac-9b1d-6d5495cef785","children":[],"parent":"308b7361-1bfb-5635-8680-19f15bde6f59","internal":{"content":"\n# Project\n\nIn Summer and Fall 2020, I was a UX Engineering intern on Google's AR team (formerly known as Daydream). Although the nature of the project is still confidential, I can still talk generally about some of the skills that I used and the lessons that I learned. If you'd like to learn more, please reach out :-). If you're a Google employee, I can put you in touch with my intern host who may be able to provide more information.\n\nTowards the end of my summer internship, my team extended my internship through a small program at Google that allows interns to continue working part-time while in university. So during the Fall semester of my senior year, I worked part-time (20% for a month, and then 50% for two months). \n\nThis was my first time working in a product-focused team. As part of the UX team, I worked closely with other UXers, but also interacted with PMs and software engineers to help investigate UX and research questions. \n\n# Technologies / skills\n1. Android app development with Android Studio\n2. Android/iOS app developemnt with Unity and AR Foundation\n3. Google Cloud Vision API integration\n4. Web development \n5. 3D modeling with Autodesk Maya\n6. Demo and concept video editing with Adobe suite\n7. Illustration under the direction of a creative director for internal projects\n8. Human-centered design thinking\n9. Prototyping and iteration\n10. System and Input UX design\n11. OpenCV\n12. Stats and research\n\n![work from home](../images/google-ar/WFH.jpg)\n<em>My desk while I worked from home!</em>\n\nThank you to my intern host Shiblee, who taught me so much about hardware UX, augmented reality, and more. He was an amazing manager! Also thank you to my co-host Wendy, an incredible designer and mentor.","type":"MarkdownRemark","contentDigest":"711a0d53006a9235f849f66f2a57439f","counter":156,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"Google AR Internship","date":"Summer/Fall 2020 (@WFH)","path":"/google-ar","author":"Lorem Ipsum","coverImage":"../images/ar-thumb.jpg","coverVideo":"../assets/ar-thumbnail.mp4","links":"","tags":["Hardware UX","Android","Unity","AR","OpenCV"],"excerpt":"Developed Android apps to research cutting-edge user experiences within Google's Devices product area.","readMoreButton":"Learn more","shortTitle":"Google AR","order":1},"excerpt":"","rawMarkdownBody":"\n# Project\n\nIn Summer and Fall 2020, I was a UX Engineering intern on Google's AR team (formerly known as Daydream). Although the nature of the project is still confidential, I can still talk generally about some of the skills that I used and the lessons that I learned. If you'd like to learn more, please reach out :-). If you're a Google employee, I can put you in touch with my intern host who may be able to provide more information.\n\nTowards the end of my summer internship, my team extended my internship through a small program at Google that allows interns to continue working part-time while in university. So during the Fall semester of my senior year, I worked part-time (20% for a month, and then 50% for two months). \n\nThis was my first time working in a product-focused team. As part of the UX team, I worked closely with other UXers, but also interacted with PMs and software engineers to help investigate UX and research questions. \n\n# Technologies / skills\n1. Android app development with Android Studio\n2. Android/iOS app developemnt with Unity and AR Foundation\n3. Google Cloud Vision API integration\n4. Web development \n5. 3D modeling with Autodesk Maya\n6. Demo and concept video editing with Adobe suite\n7. Illustration under the direction of a creative director for internal projects\n8. Human-centered design thinking\n9. Prototyping and iteration\n10. System and Input UX design\n11. OpenCV\n12. Stats and research\n\n![work from home](../images/google-ar/WFH.jpg)\n<em>My desk while I worked from home!</em>\n\nThank you to my intern host Shiblee, who taught me so much about hardware UX, augmented reality, and more. He was an amazing manager! Also thank you to my co-host Wendy, an incredible designer and mentor.","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/google_ar.md","__gatsby_resolved":{"frontmatter":{"path":"/google-ar"}}}}},"staticQueryHashes":["236058478","3128451518"]}