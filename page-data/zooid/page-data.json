{"componentChunkName":"component---src-templates-page-js","path":"/zooid","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Zooïd Lunar Gala","date":"Fall 2019 - Spring 2020","path":"/zooid","author":"Lorem Ipsum","excerpt":"A line of eight light-art wearables, made for one of Pittsburgh's largest fashion shows.","tags":null,"coverImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAgb/xAAWAQEBAQAAAAAAAAAAAAAAAAADAQL/2gAMAwEAAhADEAAAAZt9BKuXDA7/AP/EABsQAAIDAAMAAAAAAAAAAAAAAAIDAAETBBIU/9oACAEBAAEFAiZXdHLypDtgyXPKmLAQH//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAEDAQE/AYf/xAAZEQEAAgMAAAAAAAAAAAAAAAABABETITH/2gAIAQIBAT8ByCVHup//xAAdEAACAgIDAQAAAAAAAAAAAAAAAQIREjIhQVGR/9oACAEBAAY/ApLKSeT7GnFy59MqNI/DQqKpH//EABoQAAMBAQEBAAAAAAAAAAAAAAABETEhYZH/2gAIAQEAAT8hdYRdLyjawaYRHHZo3Ot7fk8/1muxYf/aAAwDAQACAAMAAAAQnx//xAAYEQADAQEAAAAAAAAAAAAAAAAAESEBMf/aAAgBAwEBPxCOGNU//8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhMf/aAAgBAgEBPxDQGTFPF//EAB4QAQEAAgEFAQAAAAAAAAAAAAERACFBMXGBkdHw/9oACAEBAAE/ED/BOhwEuR5JG6JOR74kijnbpPuJ3hVUV9Y3TVb+nIp2QV35z//Z","aspectRatio":1.7857142857142858,"src":"/static/f37bbb5989f3b5bcd92a48e8697b1d13/14b42/zooid_thumb.jpg","srcSet":"/static/f37bbb5989f3b5bcd92a48e8697b1d13/f836f/zooid_thumb.jpg 200w,\n/static/f37bbb5989f3b5bcd92a48e8697b1d13/2244e/zooid_thumb.jpg 400w,\n/static/f37bbb5989f3b5bcd92a48e8697b1d13/14b42/zooid_thumb.jpg 800w,\n/static/f37bbb5989f3b5bcd92a48e8697b1d13/47498/zooid_thumb.jpg 1200w,\n/static/f37bbb5989f3b5bcd92a48e8697b1d13/0e329/zooid_thumb.jpg 1600w,\n/static/f37bbb5989f3b5bcd92a48e8697b1d13/c7d93/zooid_thumb.jpg 3350w","sizes":"(max-width: 800px) 100vw, 800px"}}},"coverVideo":{"publicURL":"/static/0088b56e2b9d87c0c859c69b90b52cf8/zooid_shadow.mp4"},"links":""},"id":"b137c10c-c88c-5886-81f1-c54d8278992c","html":"<p>A collaboration with Alice Fang and Sebastian Carpenter.</p>\n<p>Zoöid is an exploration of light and color as a means for communication in a starless world. Inspired by deep-sea creatures that use light to communicate and coordinate, the looks in the line create an intricate lightshow on stage.</p>\n<p>This project was funded with support by Carnegie Mellon’s Undergraduate Research Office, and the Frank-Ratchye Studio for Creative Inquiry. </p>\n<h1>Show video</h1>\n<iframe class=\"video-iframe\" src=\"https://player.vimeo.com/video/423457555\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<h1>Show images</h1>\n<p><img src=\"../images/zooid_1.JPG\" alt=\"zooid cover image\">\n<img src=\"../images/zooid_2.JPG\" alt=\"zooid cover image\">\n<img src=\"../images/zooid_3.JPG\" alt=\"zooid cover image\">\n<img src=\"../images/zooid_4.JPG\" alt=\"zooid cover image\">\n<img src=\"../images/zooid_end.JPG\" alt=\"zooid cover image\"></p>\n<h1>Concept</h1>\n<p>Inspired by Pyrosomes–deep sea colonies of small animals that use light to communicate and coordinate their actions–we are interested in exploring the ways light-based communication is used to synchronize actions; to achieve this, we want to create wearables that experiment with giving humans the same capabilities. Thus, our medium of dynamic light-based wearables is critical for our exploration, and our use of physical computation, including the development of a data-transfer protocol, is critical to achieving the level of fidelity necessary for our project.</p>\n<p><img src=\"../images/moodboard.jpg\" alt=\"zooid cover image\"></p>\n<p>We are hugely inspired by prior work by researchers who create fascinating cutting-edge wearables with real-life implications, such as Electrodermis by Morphing Matter Lab and Digits Nail Technologies. Visually, we are inspired by organic forms, like the work of Iris van Herpen.  However, although there are many instances of technologists who create specifically LED wearables, we have struggled to find many compelling examples of pieces that can work collectively. We are attempting to fill a gap in trends that we have observed— although physical computing is becoming more accessible, the use of LED lights in wearables remains at a one-dimensional, ornamental level, without meaningful integration. We want to explore LED wearables from a different angle, not using the lights as the only focus but rather as a medium in which we express an underlying networked system.</p>\n<p>With clothing as part of a networked system that works collectively in communication with each other, instead of independent and isolated bodies, possibilities open up to the implications of ubiquitous technology, and the role it can play in future clothing design in the realm of performance, streetwear, disabilities resources and safety methods. For instance, we believe our system could potentially be adapted to benefit large groups working in dark or noisy environments, such as construction workers or firefighters: lights can be used to coordinate roles, and are capable of communicating across distance.</p>\n<h1>Process images</h1>\n<p>For the process of garment construction, base garments will be constructed with traditional sewing techniques, using tulle, spandex, cotton, and chiffon. The structural parts that house electronics will be created from 3D-printed flexible plastic that will allow us to print flat pieces that can conform to the model’s body. Our outfits use Arduinos and XBees, which allow each outfit to wirelessly communicate with a computer, which processes incoming data, such as music. Based on that data, color schemes and intensities are sent to XBee on each outfit, which receives the color scheme and intensity. We will develop our own protocol for data transfer in order to avoid the computer receiving multiple signals that interfere with each other. Colors change according to instructions received by the XBee, and the outfit will be lit with Adafruit NeoPixel LEDs. The lights will be dispersed through tulle fabric, fiber-optics, or acrylic plastic. </p>\n<p><img src=\"../images/zooid-process3.jpg\" alt=\"zooid cover image\">\n<img src=\"../images/zooid-process4.jpg\" alt=\"zooid cover image\">\n<img src=\"../images/zooid-process5.jpg\" alt=\"zooid cover image\">\n<img src=\"../images/zooid-process6.jpg\" alt=\"zooid cover image\"></p>","excerpt":"A collaboration with Alice Fang and Sebastian Carpenter. Zoöid is an exploration of light and color as a means for communication in a…"}},"pageContext":{"type":"posts","next":{"id":"7164d671-d395-51d5-905f-6da2cd89f39f","children":[],"parent":"46647fce-2732-5124-a8e1-d6c1e4810450","internal":{"content":"\n<div style=\"display:flex; flex-direction:row; justify-content:space-between;\">\n\n<video autoplay loop muted style=\"width: 35%;\" poster=\"../images/ar_phone_thumb.jpg\">\n    <source src=\"../images/ar_phone_2.mp4\">\n</video>\n<div>\n<p> As a research assistant for the Studio for Creative Inquiry, I created nine simple Unity demos for students in a creative computation course at Carnegie Mellon to use as a reference for their projects. </p>\n\n<h2>ARKit 2 Demos</h2>\n\nPlease view <a href=\"https://github.com/CreativeInquiry/ARKit-Educational-Templates\">this github repo</a> to view the work and more information.\n\n<h2>ARCore Demos</h2>\nPlease view <a href=\"https://github.com/CreativeInquiry/ARCore-Educational-Templates/\">this github repo</a> to view the work and more information.\n</div>\n</div>","type":"MarkdownRemark","contentDigest":"957a37d2d366b29efe48923998b59055","counter":96,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"Augmented Reality Educational Templates","path":"/ar-templates","date":"Fall 2019","coverImage":"../images/ar_phone_thumb.jpg","coverVideo":"../assets/ar_phone.mp4","author":"Elliot","excerpt":"Nine simple Unity demos for students in a creative computation course at Carnegie Mellon to use as a reference for their projects.","tags":["rob____ot","hello friend"],"links":"iOS templates,https://google.com;Android templates,https://google.com","readMoreButton":"More info","order":3},"excerpt":"","rawMarkdownBody":"\n<div style=\"display:flex; flex-direction:row; justify-content:space-between;\">\n\n<video autoplay loop muted style=\"width: 35%;\" poster=\"../images/ar_phone_thumb.jpg\">\n    <source src=\"../images/ar_phone_2.mp4\">\n</video>\n<div>\n<p> As a research assistant for the Studio for Creative Inquiry, I created nine simple Unity demos for students in a creative computation course at Carnegie Mellon to use as a reference for their projects. </p>\n\n<h2>ARKit 2 Demos</h2>\n\nPlease view <a href=\"https://github.com/CreativeInquiry/ARKit-Educational-Templates\">this github repo</a> to view the work and more information.\n\n<h2>ARCore Demos</h2>\nPlease view <a href=\"https://github.com/CreativeInquiry/ARCore-Educational-Templates/\">this github repo</a> to view the work and more information.\n</div>\n</div>","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/ar_templates.md"},"previous":{"id":"5d3433e6-2945-58dc-97a9-725b0bbf7073","children":[],"parent":"f5f5b024-afb4-558a-9cb5-54bcf0d9a3b4","internal":{"content":"# About\nMldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.\nMldraw is a web app that uses a layered vector drawing system where each layer can be given a different machine learning model that translates user input. The user will give us a line drawing of edges, and our app's backend server renders the translation using whatever model is assigned to that layer.\nFurthermore, we've also made it easy to import custom ML models for researchers so that they can use our tool to experiment with their models!\nMade in collaboration with [Aman Tiwari](http://www.aman.work/)!\nI designed and built the UI using HTML/CSS/Typescript, and I also worked on the drawing and layer functionality, such as the\nvector drawing, drag and dropping objects, layer building, etc.\nMldraw’s interface is inspired by cute, techy/anti-techy retro aesthetics, such as the work of Sailor Mercury and the Bubblesort Zines.  We wanted it to be fun, novel, exciting and differentiated from the world of arxiv papers.\nMldraw was born out of seeing the potential of the body of research done using pix2pix to turn drawings into other images and the severe lack of a usable, “useful” and accessible tool to utilize this technology.\n\n# Videos\n\n<iframe class=\"iframe-560h\" src=\"https://player.vimeo.com/video/321893512\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<iframe class=\"iframe-620h\" src=\"https://player.vimeo.com/video/378759111\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Still in progress\nWe have a great demo, but have plans to bug-test, add more exciting ML models and features, and deploy at mldraw.com. We're also still experimenting and making big changes to the UI / UX! More documentation coming soon.\n![edges2pikachu example](../images/mldraw_beforeafter.jpg) *Left: User line drawing, Right: rendered version. Combined pikachu model and handbag model*\n\n# Implementation\nMldraw is implemented as a Typescript frontend using choo.js as a UI framework, with a Python registry server and a Python adapter library, along with a number of instantiations of the adapter library for specific models.\nThe frontend communicates with the registry server using socket.io, which then passes to the frontend a list of models and their URLs. The frontend then communicates directly to the models.  This enables us e.g. to host a registry server for Mldraw without having to pay the cost of hosting every model it supports.\nMldraw also supports models that run locally on the client (in the above video, the cat, Pikachu and bag models run locally, while the other models are hosted on remote servers).\nIn service of the above desire to make Mldraw extensible, we have made it easy to add a new model – all that is required is some Python interface to the model, and to define a function that takes in an image and returns an image. Our model adapter will handle the rest of it, including registering the model with the server hosting a Mldraw interface.\n\n# Progress Documentation\n\n<video autoplay loop muted poster=\"mldraw_sequence.jpg\">\n    <source src=\"../images/mldraw_sequence.mp4\">\n</video>\n\n## CSS animations\n![render button interaction gif](../images/button-interactions.gif)*Examples of small css animated interactions, Left: tooltip + pop animation on hover + lingering expansion until unhover, Middle: render button disappears when canvas in use, fades back in when done, Right: render button spins while rendering*\n\n\n\n![toolbar interaction gif](../images/toolbar.gif)*toolbar animations: nod when usable, shake when unusable for that model*\n","type":"MarkdownRemark","contentDigest":"ca7eab01386398ed8281a449a1b90227","counter":105,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"mldraw Drawing Tool","path":"/mldraw","date":"Spring 2019 - Spring 2020","coverImage":"../images/mldraw_thumb.jpg","coverVideo":"../assets/mldraw.mp4","author":"Elliot","excerpt":"Mldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.","tags":["rob____ot","hello friend"],"links":"","readMoreButton":"See documentation","order":1},"excerpt":"","rawMarkdownBody":"# About\nMldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.\nMldraw is a web app that uses a layered vector drawing system where each layer can be given a different machine learning model that translates user input. The user will give us a line drawing of edges, and our app's backend server renders the translation using whatever model is assigned to that layer.\nFurthermore, we've also made it easy to import custom ML models for researchers so that they can use our tool to experiment with their models!\nMade in collaboration with [Aman Tiwari](http://www.aman.work/)!\nI designed and built the UI using HTML/CSS/Typescript, and I also worked on the drawing and layer functionality, such as the\nvector drawing, drag and dropping objects, layer building, etc.\nMldraw’s interface is inspired by cute, techy/anti-techy retro aesthetics, such as the work of Sailor Mercury and the Bubblesort Zines.  We wanted it to be fun, novel, exciting and differentiated from the world of arxiv papers.\nMldraw was born out of seeing the potential of the body of research done using pix2pix to turn drawings into other images and the severe lack of a usable, “useful” and accessible tool to utilize this technology.\n\n# Videos\n\n<iframe class=\"iframe-560h\" src=\"https://player.vimeo.com/video/321893512\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<iframe class=\"iframe-620h\" src=\"https://player.vimeo.com/video/378759111\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Still in progress\nWe have a great demo, but have plans to bug-test, add more exciting ML models and features, and deploy at mldraw.com. We're also still experimenting and making big changes to the UI / UX! More documentation coming soon.\n![edges2pikachu example](../images/mldraw_beforeafter.jpg) *Left: User line drawing, Right: rendered version. Combined pikachu model and handbag model*\n\n# Implementation\nMldraw is implemented as a Typescript frontend using choo.js as a UI framework, with a Python registry server and a Python adapter library, along with a number of instantiations of the adapter library for specific models.\nThe frontend communicates with the registry server using socket.io, which then passes to the frontend a list of models and their URLs. The frontend then communicates directly to the models.  This enables us e.g. to host a registry server for Mldraw without having to pay the cost of hosting every model it supports.\nMldraw also supports models that run locally on the client (in the above video, the cat, Pikachu and bag models run locally, while the other models are hosted on remote servers).\nIn service of the above desire to make Mldraw extensible, we have made it easy to add a new model – all that is required is some Python interface to the model, and to define a function that takes in an image and returns an image. Our model adapter will handle the rest of it, including registering the model with the server hosting a Mldraw interface.\n\n# Progress Documentation\n\n<video autoplay loop muted poster=\"mldraw_sequence.jpg\">\n    <source src=\"../images/mldraw_sequence.mp4\">\n</video>\n\n## CSS animations\n![render button interaction gif](../images/button-interactions.gif)*Examples of small css animated interactions, Left: tooltip + pop animation on hover + lingering expansion until unhover, Middle: render button disappears when canvas in use, fades back in when done, Right: render button spins while rendering*\n\n\n\n![toolbar interaction gif](../images/toolbar.gif)*toolbar animations: nod when usable, shake when unusable for that model*\n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/mldraw.md"}}},"staticQueryHashes":["236058478","3128451518"]}