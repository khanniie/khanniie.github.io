{"componentChunkName":"component---src-templates-page-js","path":"/sephora","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Sephora","date":"Fall 2020","path":"/sephora","author":null,"excerpt":"A dataset of Sephora reviews that mention tears, crying or sobbing.","tags":["Frontend","web","Data scraping"],"coverImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3lhQf//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEAAQUCX//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABUQAQEAAAAAAAAAAAAAAAAAAAEg/9oACAEBAAE/IWv/2gAMAwEAAgADAAAAECAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGBAAAwEBAAAAAAAAAAAAAAAAAAERIYH/2gAIAQEAAT8QdzLwTeFINR4JYf/Z","aspectRatio":1.7699115044247788,"src":"/static/fcb9a6c286232f7b70875f121f57fd44/14b42/sephora_thumb.jpg","srcSet":"/static/fcb9a6c286232f7b70875f121f57fd44/f836f/sephora_thumb.jpg 200w,\n/static/fcb9a6c286232f7b70875f121f57fd44/2244e/sephora_thumb.jpg 400w,\n/static/fcb9a6c286232f7b70875f121f57fd44/14b42/sephora_thumb.jpg 800w,\n/static/fcb9a6c286232f7b70875f121f57fd44/a7715/sephora_thumb.jpg 1000w","sizes":"(max-width: 800px) 100vw, 800px"}}},"coverVideo":{"publicURL":"/static/505cce413fd3b818cddbf2bea382bb49/sephora.mp4"},"links":"on Github,https://github.com/everestpipkin/datagardens/tree/master/students/khanniie/5_newDataSet;On Mashable,https://mashable.com/article/sephora-crying-reviews-data-set/;On Flowing Data,https://flowingdata.com/2019/11/07/sephora-dataset-is-a-collection-of-makeup-reviews-that-mention-crying/"},"id":"09d2c2ab-4356-52d6-9198-fc2e2fbc2f9e","html":"<h1>Concept</h1>\n<p class=\"para\">In high school, I spent a lot of time reading Sephora reviews in pursuit of the perfect object because I was afraid of wasting my money. When I read the prompt for this dataset, I remembered how extensive the data on Sephora's webiste had been.</p>\n<p class=\"para\">A popular metric for a successful waterproof product is whether or not it can withstand tears and emotional turmoil. I remembered seeing a review giving an eyeliner 5 stars for surviving through a teary breakup, and I wanted to see if there would be more. Thus, for this project, I scraped Sephora's website for reviews, and filtered through them for reviews mentioning crying.</p>\n<p class=\"para\">I ended up scraping about ~5k reviews, and 105 of them mentioned crying, sobbing or tears, giving a ratio of about 1/50. This is of course a biased number because the products the reviews are for are meant to withstand water, but I was still surprised to find so many. I was also surprised by how confessional and emotional people were willing to be in their reviews; I saw stories about breakups, death of loved ones, weddings, fights and more. However, despite the tragedy underlying many of the stories, the tone was often strangely positive, providing exuberant praise for the product that allowed them to maintain their makeup throughout the tragedy.</p>\n<h1>Online viewer: <a href=\"https://connie.dog/sephora\">connie.dog/sephora</a></h1>\n<iframe src=\"https://connie.dog/sephora/\"></iframe>\n<h1>See the dataset on <a href=\"https://github.com/everestpipkin/datagardens/tree/master/students/khanniie/5_newDataSet\">Github</a></h1>\n<h1>Stats:</h1>\n<pre><code>1. 5018 total reviews\n2. 13 different products\n    a. 5 mascara\n    b. 5 eyeliners\n    c. 1 face primer\n    d. 1 eye primer\n    e. 1 setting spray\n3. 105 total crying reviews found\n</code></pre>\n<h1>In the media</h1>\n<p class=\"para\">\"Reading [the reviews] is a frequently funny and occasionally poignant experience.\"</p>\n<h2><a href=\"https://mashable.com/article/sephora-crying-reviews-data-set/\">on Mashable</a></h2>\n<h2><a href=\"https://flowingdata.com/2019/11/07/sephora-dataset-is-a-collection-of-makeup-reviews-that-mention-crying/\">on Flowing Data</a></h2>\n<h2><a href=\"https://twitter.com/crabbage_/status/1190790979709755392\">on Twitter</a></h2>","excerpt":"Concept In high school, I spent a lot of time reading Sephora reviews in pursuit of the perfect object because I was afraid of wasting my…"}},"pageContext":{"type":"posts","next":{"id":"6ba4fc96-e6c7-5f70-ae5d-2fb390bdc133","children":[],"parent":"905306bb-d9f6-5876-898c-8eab374cde1d","internal":{"content":"\n# TartanHacks: Oink Accessibility Chrome extension\n<p class=\"date\">Spring 2019</p>\n\n**A collaboration between Megan Ung, Kearnie Lin and Kimberly Lo**\n\nThis project won the Google Accessibility Award at TartanHacks. It's a chrome extension that uses computer vision to auto-generate missing alt tags for images, hide sensitive content, etc.\n\nOink won the Google Accessibility Award at TartanHacks! Oink is a Chrome extension with a suite of web accessibility tools. It uses computer vision to auto-generate missing alt tags for images, hides sensitive content, makes font sizes bigger, and alerts users when sound is\nplaying on a webpage. All of these functionalities are included, but can be toggled on and off based on user needs.\nFor this project, I worked in a team of 4, with Kearnie Lin, Megan Ung and Kimberly Lo. I designed and built the UI; I also set up the socket.io backend that's being used to perform the calls to Google Cloud Vision.\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/oink/oink1.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/oink/oink2.png\"/></div>\n</div>\n<em>left: popup menu in chrome, right: our icon in the toolbar</em>\n\n![image of interface](../images/oink/oink6.png)*image of interface*\n## Adding in missing alt tags\nAlt tags are important components of images on the internet for those who use screenreaders to browse the web. However, websites sometimes don't include alt tags for these images, leaving the screenreader-reliant user unable to\nknow what the content of the image is. We realized that we could use modern computer vision techniques to remedy this; our chrome extension looks for images that are missing alt tags, and sends those images\nto our backend through socket.io, which then sends the image through Google Cloud's computer vision to be labeled. We then add in these content labels as alt tags. This socket.io backend was being run continuously on my\nDigital Ocean server.\nBelow is a webpage without our extension. It doesn't have alt tags on its images. With our extension turned on, alt tags have been applied to the image!\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/oink/oink8.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/oink/oink7.png\"/></div>\n</div>\n\n## Sensitive Content Filter\nUsing the same techniques as described above, we can determine what the content of an image is. The web can be difficult for those who are especially sensitive to certain content,\nso we allow the user to give us a list of tags to watch for. We will then hide images that match those tags.\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/oink/oink6.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/oink/oink5.png\"/></div>\n</div>\n\n## More!\nOur app can also increase font size, make links more visible, and alert you if there's sound playing on a page.\n\n## Future Goals\n![future goals](../images/oink/oink-future.png)*future goals*\n## At the Hackathon!\n![presenting](../images/oink/tartanhacks-prog.jpg)*Our team presenting our project!*\n\n<div class=\"br\"></div>\n<hr/>\n\n# Pixel Art TSNE and DCGAN\n<p class=\"date\">Fall 2018</p>\n\n**I datascraped ~5000 pixel icons from Deviantart and then extracted feature vectors from them, which were used to put them into a grid of nearest neighbors. I also used DCGAN to generate new pixel icons.**\n\n## Inspiration\nWith this project, I wanted to computationally analyze and recreate Deviantart art because I had been a very active member of Deviantart in my middle school days and\nthought that it would be a nice tribute to those days. I choose pixel icons specifically because of their uniform style and nostalgic value (for me, at least). Because a lot of the work was a little computationally heavy, I remotely ran most of\nmy code on a computer hosted in CMU's Studio for Creative Inquiry. I had been reading a good amount of papers about machine learning and wanted to really play around with topics related to what I had been reading about. I created this project during the first third of my independent study class.\n## Scraping for Images\nI used Node.js to grab roughly 5000 images using the Deviantart API, querying for square pixel images with a certain amount of likes that had been marked by their creator as \"downloadable.\"\n![deviantart pixel](../images/pixel/devpix.gif)\n## Feature extraction and Tsne\nIn a Jupyter Notebook, I used Pytorch's pretrained VGG16 model to extract feature vectors from all of my images. This allowed me to get the\n\"nearest neighbors\" of any given pixel icon. Shown below are examples of the nearest neighbor finding. The top/larger image was the origin image and the\nfive images below are its nearest neighbors.\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/pixel/neighbor1.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/neighbor2.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/neighbor3.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/neighbor4.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/neighbor5.png\"/></div>\n</div>\n\nThen, I could use run tsne on these images to plot them by feature vector. I also used\nan updated (Python 3) implementation of Raster Fairy to turn the 2D point cloud into a grid. Below is the result on a smaller (2000 images) subset of pixels.\n![tsne](../images/pixel/2000_set_grey.png)\n\n## DCGAN\nI used an implementation of DCGAN that I found [here](https://github.com/eriklindernoren/PyTorch-GAN) to generate more.\nThey turned out looking a little like pixel mush, so I tried messing around with the learning rate / number of epochs. The results still weren't as good as I was hoping,\nbut I learned a lot about GANs and machine learning during this whole process and am really happy that I did this project.\n\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/pixel/dcgan1.gif\" alt=\"dcgan res 1\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/dcgan2.gif\" alt=\"dcgan res 2\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/dcgan-big.gif\"/></div>\n</div>\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/pixel/60800.png\" alt=\"dcgan res 1\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/combined-dcgan.jpg\" alt=\"dcgan res 2\"/></div>\n</div>\n\n<div class=\"br-medium\"></div>\n<hr/>\n\n# HackMIT: ResQ \n<p class=\"date\">Fall 2018</p>\n\nWon HackMIT IBM sponsor award. A all-in-one disaster preparedness + relief tool for both individuals and rescue orgs.\n\n## About\nMade at 2018 HackMIT, won the Best Disaster Preparedness & Relief Solutions for IBM Call for Code Challenge award!\nAn all-in-one disaster preparedness + relief tool.\nPeople can access free information about potential and\nongoing disasters as well as giving rescue teams and aid organizations to assess needs\nfor each specific event using Google analytics and location data.\nIt has a working implementation of an IBM Watson natural language processing data pipeline\nthat analyses real-time Twitter data to predict disaster damage better that the Federal Emergency Management Agency (FEMA).\nI made the user registration / login flow, the user dashboard and on the Firebase user information storing and retrieval.\n## DEVPOST POST [HERE](https://devpost.com/software/disaster-rescue-github-io)\n\n<hr/>\n\n# Secret Base AR\n<p class=\"date\">Spring 2018</p>\n\n**Using distinctive images (posters, stickers, wall patterns) from real life, users can build “secret bases” that will pop up any time the app detects those images. These bases will be stored and can be sent to the users’ friends through a friend code.** \n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Vk_ZJ2VbFUA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## About\nUsing distinctive images (posters, stickers, wall patterns) from real life, users can build “secret bases” that will pop up any time the app detects those images. These bases will be stored and can be sent to the users’ friends through a friend code. These friends also have the ability to see and modify the same structure if they are within proximity of the same image.\nI created this project to explore the idea of digital intimacy and friendship; I was interested in internet-based relationship dynamics and wanted to prototype a way to share a personal space digitally.\nAll the furniture was textured with hand-painted watercolor textures, and the “sharing” aspect uses both Firebase authorization for user sign in and Firebase’s database to remember base information. The AR was made using vuforia’s cloud database and extended tracking.\n\n![image](../images/secretbase/secretbase.jpg)*furniture in Unity environment*\n\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/secretbase/ar-secret-base.gif\"/></div>\n    <img class=\"flex-item\" src=\"../images/secretbase/sb-collab.gif\"/>\n</div>\n<em>left: AR tree and tables, right: treehouse shared with friend's phone, both can view and edit the same structure</em>\n\n<div class=\"br\"></div>\n\n## The Process\nI started with concept artwork. With AR secret bases, the bases are located forever where their \"image target\" exists and are\nonly accessible to those who have been invited, kind of like a secret treehouse club.\n![concept art](../images/secretbase/arbase-concept.jpg)\nI began by working on integrating Firebase user auth and database into Unity. \n\nI then started building the Unity environment, which I wanted to use my own textures for. I had considered using premade assets, but wanted\nmy final result to have a more distinctive look. I watercolored my textures in my sketchbook, scanned them in, made a bump map using photoshop and then textured the objects.\n\nFor the basic grid and block placing implementation, the blocks had to snap to the grid and be able to be stacked upon each other. Furthermore, I wanted to have a square\nthat would indicate where the next block would be placed down if the user would press \"place.\"\n\n<div class=\"fill-row\">\n<div class=\"flex-item-start\"><img src=\"../images/secretbase/firebase.png\"/></div>\n<div class=\"flex-item\"><img src=\"../images/secretbase/brick-demo.gif\"/></div>\n</div>\n\nI then integrated the block system into the AR part. For each image target I loaded into Vuforia, I gave it a unique id that would later be used to map the image target to them\nfurniture data stored in Firebase.\n\nThen, I worked on getting the firebase details correct. I was able to send and retrieve data, and authorize users, but I didn’t have a structured way of implementing my idea yet.\n\nOverall, this was a hard and time-consuming project for me, but I learned a lot about Unity, AR and Firebase in the process.\n\nAlthough it could be cleaned up a lot, I was really happy with how it turned out and was happily surprised by how functional it was.\n\nThank you to: Nitesh for helping me document, Rain for being the best 15-251 partner, Sophia C. for being a Very Good Buddy, Golan Levin for providing wonderful advice, and Claire for her advice on the watercolor textures!\n\n<div class=\"br-medium\"></div>\n<hr/>\n\n\n# Weekly deliverables\n<p class=\"date\">Spring 2018</p>\n\n**For the first half of Golan Levin's Spring 2018 Interactivity and Computation course, we produced weekly deliverables.**\n\n## Aesemic Writing (maze writing)\nFor this assignment, I imagined an alien civilization that spoke in riddles and wrote in mazes. I created a \"maze language\" where each letter is mapped to a maze fragment and put together.\nThe resulting \"maze\" is then carved using the union find algorithm so that it will be a real maze. My resulting mazes were drawn onto paper using the Axidraw plotter.\n<div id=\"illustrations\">\n    <img src=\"../images/60212/aesemic3.jpg\"/>\n    <img src=\"../images/60212/aesemic2.gif\"/>\n    <img src=\"../images/60212/aesemic3.gif\"/>\n</div>\n<em>Process sketches / pseudocode, including the conversion from letter to maze piece</em>\n\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/60212/pathcarving.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/60212/pathcarving-after.png\"/></div>\n</div>\n\n## Speech (Word sentiment flower)\nUsing a speech sentiment library and p5.speech, I analyze the sentiment of\nthe user's speech and the flower will grow or wilt according to the sentiment. For example,\nbad dirt\" will cause it to wilt while \"happy flower\" will cause it to grow.\nI also used rita.js to give extra impact for rhymes. The flower is drawn using p5.js!\n![flower](../images/60212/flower.gif)\n\n## Motion Capture Metaballs\nI hooked up mocap data to a three.js metaball demo to create mocap metaball figures!\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/60212/mocap.gif\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/60212/mocap2.gif\"/></div>\n</div>\n\n## Animated Loop\nAn animated loop made in Processing.\n![animated loop](../images/60212/animated_loop.gif)\n\n## Caffeine Bot\nCaffeine bot tracks how long you have until you need to recaffinate!\n![Caffeine bot](../images/60212/caffine_bot.gif)\n\n<div class=\"br-medium\"></div>\n<hr/>\n\n# Bot-a-razzi\n<p class=\"date\">Fall 2017</p>\n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/R-ySfk5PLh8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nFinal concept studio project; worked in a team of 4. Floating robots follow the user around in this VR experience built for the Google Cardboard using Aframe.js. The robot \"view\" info is sent to a different screen that projects a fake instagram profile with the video stream. I worked on the projections, sending and retrieving the data using Firebase. I also 3d modeled some of the models included and built the fake social media pages and their fake chat streams.\n\n![botarazzi image 1](../images/botarazzi/cityconcept-thumb.jpg)*above: concept art by me for bot-a-razzi.*\n![botarazzi image 1](../images/botarazzi/bot-a-razzi-long.png)*above: baby room (there are three rooms) in bot-a-razzi.*\n","type":"MarkdownRemark","contentDigest":"fc8b6c6935b8ecb1ea4d7b377bc7196c","counter":164,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"Other work","path":"/other","date":"","coverImage":"../images/old_work.jpg","coverVideo":"../assets/old_work.mp4","excerpt":"A collection of older or shorter projects.","tags":["AR","Unity","Chrome extension","p5.js","Processing","ML"],"links":"","readMoreButton":"See older projects","order":11,"shortTitle":"Other work"},"excerpt":"","rawMarkdownBody":"\n# TartanHacks: Oink Accessibility Chrome extension\n<p class=\"date\">Spring 2019</p>\n\n**A collaboration between Megan Ung, Kearnie Lin and Kimberly Lo**\n\nThis project won the Google Accessibility Award at TartanHacks. It's a chrome extension that uses computer vision to auto-generate missing alt tags for images, hide sensitive content, etc.\n\nOink won the Google Accessibility Award at TartanHacks! Oink is a Chrome extension with a suite of web accessibility tools. It uses computer vision to auto-generate missing alt tags for images, hides sensitive content, makes font sizes bigger, and alerts users when sound is\nplaying on a webpage. All of these functionalities are included, but can be toggled on and off based on user needs.\nFor this project, I worked in a team of 4, with Kearnie Lin, Megan Ung and Kimberly Lo. I designed and built the UI; I also set up the socket.io backend that's being used to perform the calls to Google Cloud Vision.\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/oink/oink1.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/oink/oink2.png\"/></div>\n</div>\n<em>left: popup menu in chrome, right: our icon in the toolbar</em>\n\n![image of interface](../images/oink/oink6.png)*image of interface*\n## Adding in missing alt tags\nAlt tags are important components of images on the internet for those who use screenreaders to browse the web. However, websites sometimes don't include alt tags for these images, leaving the screenreader-reliant user unable to\nknow what the content of the image is. We realized that we could use modern computer vision techniques to remedy this; our chrome extension looks for images that are missing alt tags, and sends those images\nto our backend through socket.io, which then sends the image through Google Cloud's computer vision to be labeled. We then add in these content labels as alt tags. This socket.io backend was being run continuously on my\nDigital Ocean server.\nBelow is a webpage without our extension. It doesn't have alt tags on its images. With our extension turned on, alt tags have been applied to the image!\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/oink/oink8.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/oink/oink7.png\"/></div>\n</div>\n\n## Sensitive Content Filter\nUsing the same techniques as described above, we can determine what the content of an image is. The web can be difficult for those who are especially sensitive to certain content,\nso we allow the user to give us a list of tags to watch for. We will then hide images that match those tags.\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/oink/oink6.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/oink/oink5.png\"/></div>\n</div>\n\n## More!\nOur app can also increase font size, make links more visible, and alert you if there's sound playing on a page.\n\n## Future Goals\n![future goals](../images/oink/oink-future.png)*future goals*\n## At the Hackathon!\n![presenting](../images/oink/tartanhacks-prog.jpg)*Our team presenting our project!*\n\n<div class=\"br\"></div>\n<hr/>\n\n# Pixel Art TSNE and DCGAN\n<p class=\"date\">Fall 2018</p>\n\n**I datascraped ~5000 pixel icons from Deviantart and then extracted feature vectors from them, which were used to put them into a grid of nearest neighbors. I also used DCGAN to generate new pixel icons.**\n\n## Inspiration\nWith this project, I wanted to computationally analyze and recreate Deviantart art because I had been a very active member of Deviantart in my middle school days and\nthought that it would be a nice tribute to those days. I choose pixel icons specifically because of their uniform style and nostalgic value (for me, at least). Because a lot of the work was a little computationally heavy, I remotely ran most of\nmy code on a computer hosted in CMU's Studio for Creative Inquiry. I had been reading a good amount of papers about machine learning and wanted to really play around with topics related to what I had been reading about. I created this project during the first third of my independent study class.\n## Scraping for Images\nI used Node.js to grab roughly 5000 images using the Deviantart API, querying for square pixel images with a certain amount of likes that had been marked by their creator as \"downloadable.\"\n![deviantart pixel](../images/pixel/devpix.gif)\n## Feature extraction and Tsne\nIn a Jupyter Notebook, I used Pytorch's pretrained VGG16 model to extract feature vectors from all of my images. This allowed me to get the\n\"nearest neighbors\" of any given pixel icon. Shown below are examples of the nearest neighbor finding. The top/larger image was the origin image and the\nfive images below are its nearest neighbors.\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/pixel/neighbor1.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/neighbor2.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/neighbor3.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/neighbor4.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/neighbor5.png\"/></div>\n</div>\n\nThen, I could use run tsne on these images to plot them by feature vector. I also used\nan updated (Python 3) implementation of Raster Fairy to turn the 2D point cloud into a grid. Below is the result on a smaller (2000 images) subset of pixels.\n![tsne](../images/pixel/2000_set_grey.png)\n\n## DCGAN\nI used an implementation of DCGAN that I found [here](https://github.com/eriklindernoren/PyTorch-GAN) to generate more.\nThey turned out looking a little like pixel mush, so I tried messing around with the learning rate / number of epochs. The results still weren't as good as I was hoping,\nbut I learned a lot about GANs and machine learning during this whole process and am really happy that I did this project.\n\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/pixel/dcgan1.gif\" alt=\"dcgan res 1\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/dcgan2.gif\" alt=\"dcgan res 2\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/dcgan-big.gif\"/></div>\n</div>\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/pixel/60800.png\" alt=\"dcgan res 1\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/pixel/combined-dcgan.jpg\" alt=\"dcgan res 2\"/></div>\n</div>\n\n<div class=\"br-medium\"></div>\n<hr/>\n\n# HackMIT: ResQ \n<p class=\"date\">Fall 2018</p>\n\nWon HackMIT IBM sponsor award. A all-in-one disaster preparedness + relief tool for both individuals and rescue orgs.\n\n## About\nMade at 2018 HackMIT, won the Best Disaster Preparedness & Relief Solutions for IBM Call for Code Challenge award!\nAn all-in-one disaster preparedness + relief tool.\nPeople can access free information about potential and\nongoing disasters as well as giving rescue teams and aid organizations to assess needs\nfor each specific event using Google analytics and location data.\nIt has a working implementation of an IBM Watson natural language processing data pipeline\nthat analyses real-time Twitter data to predict disaster damage better that the Federal Emergency Management Agency (FEMA).\nI made the user registration / login flow, the user dashboard and on the Firebase user information storing and retrieval.\n## DEVPOST POST [HERE](https://devpost.com/software/disaster-rescue-github-io)\n\n<hr/>\n\n# Secret Base AR\n<p class=\"date\">Spring 2018</p>\n\n**Using distinctive images (posters, stickers, wall patterns) from real life, users can build “secret bases” that will pop up any time the app detects those images. These bases will be stored and can be sent to the users’ friends through a friend code.** \n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Vk_ZJ2VbFUA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## About\nUsing distinctive images (posters, stickers, wall patterns) from real life, users can build “secret bases” that will pop up any time the app detects those images. These bases will be stored and can be sent to the users’ friends through a friend code. These friends also have the ability to see and modify the same structure if they are within proximity of the same image.\nI created this project to explore the idea of digital intimacy and friendship; I was interested in internet-based relationship dynamics and wanted to prototype a way to share a personal space digitally.\nAll the furniture was textured with hand-painted watercolor textures, and the “sharing” aspect uses both Firebase authorization for user sign in and Firebase’s database to remember base information. The AR was made using vuforia’s cloud database and extended tracking.\n\n![image](../images/secretbase/secretbase.jpg)*furniture in Unity environment*\n\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/secretbase/ar-secret-base.gif\"/></div>\n    <img class=\"flex-item\" src=\"../images/secretbase/sb-collab.gif\"/>\n</div>\n<em>left: AR tree and tables, right: treehouse shared with friend's phone, both can view and edit the same structure</em>\n\n<div class=\"br\"></div>\n\n## The Process\nI started with concept artwork. With AR secret bases, the bases are located forever where their \"image target\" exists and are\nonly accessible to those who have been invited, kind of like a secret treehouse club.\n![concept art](../images/secretbase/arbase-concept.jpg)\nI began by working on integrating Firebase user auth and database into Unity. \n\nI then started building the Unity environment, which I wanted to use my own textures for. I had considered using premade assets, but wanted\nmy final result to have a more distinctive look. I watercolored my textures in my sketchbook, scanned them in, made a bump map using photoshop and then textured the objects.\n\nFor the basic grid and block placing implementation, the blocks had to snap to the grid and be able to be stacked upon each other. Furthermore, I wanted to have a square\nthat would indicate where the next block would be placed down if the user would press \"place.\"\n\n<div class=\"fill-row\">\n<div class=\"flex-item-start\"><img src=\"../images/secretbase/firebase.png\"/></div>\n<div class=\"flex-item\"><img src=\"../images/secretbase/brick-demo.gif\"/></div>\n</div>\n\nI then integrated the block system into the AR part. For each image target I loaded into Vuforia, I gave it a unique id that would later be used to map the image target to them\nfurniture data stored in Firebase.\n\nThen, I worked on getting the firebase details correct. I was able to send and retrieve data, and authorize users, but I didn’t have a structured way of implementing my idea yet.\n\nOverall, this was a hard and time-consuming project for me, but I learned a lot about Unity, AR and Firebase in the process.\n\nAlthough it could be cleaned up a lot, I was really happy with how it turned out and was happily surprised by how functional it was.\n\nThank you to: Nitesh for helping me document, Rain for being the best 15-251 partner, Sophia C. for being a Very Good Buddy, Golan Levin for providing wonderful advice, and Claire for her advice on the watercolor textures!\n\n<div class=\"br-medium\"></div>\n<hr/>\n\n\n# Weekly deliverables\n<p class=\"date\">Spring 2018</p>\n\n**For the first half of Golan Levin's Spring 2018 Interactivity and Computation course, we produced weekly deliverables.**\n\n## Aesemic Writing (maze writing)\nFor this assignment, I imagined an alien civilization that spoke in riddles and wrote in mazes. I created a \"maze language\" where each letter is mapped to a maze fragment and put together.\nThe resulting \"maze\" is then carved using the union find algorithm so that it will be a real maze. My resulting mazes were drawn onto paper using the Axidraw plotter.\n<div id=\"illustrations\">\n    <img src=\"../images/60212/aesemic3.jpg\"/>\n    <img src=\"../images/60212/aesemic2.gif\"/>\n    <img src=\"../images/60212/aesemic3.gif\"/>\n</div>\n<em>Process sketches / pseudocode, including the conversion from letter to maze piece</em>\n\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/60212/pathcarving.png\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/60212/pathcarving-after.png\"/></div>\n</div>\n\n## Speech (Word sentiment flower)\nUsing a speech sentiment library and p5.speech, I analyze the sentiment of\nthe user's speech and the flower will grow or wilt according to the sentiment. For example,\nbad dirt\" will cause it to wilt while \"happy flower\" will cause it to grow.\nI also used rita.js to give extra impact for rhymes. The flower is drawn using p5.js!\n![flower](../images/60212/flower.gif)\n\n## Motion Capture Metaballs\nI hooked up mocap data to a three.js metaball demo to create mocap metaball figures!\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/60212/mocap.gif\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/60212/mocap2.gif\"/></div>\n</div>\n\n## Animated Loop\nAn animated loop made in Processing.\n![animated loop](../images/60212/animated_loop.gif)\n\n## Caffeine Bot\nCaffeine bot tracks how long you have until you need to recaffinate!\n![Caffeine bot](../images/60212/caffine_bot.gif)\n\n<div class=\"br-medium\"></div>\n<hr/>\n\n# Bot-a-razzi\n<p class=\"date\">Fall 2017</p>\n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/R-ySfk5PLh8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nFinal concept studio project; worked in a team of 4. Floating robots follow the user around in this VR experience built for the Google Cardboard using Aframe.js. The robot \"view\" info is sent to a different screen that projects a fake instagram profile with the video stream. I worked on the projections, sending and retrieving the data using Firebase. I also 3d modeled some of the models included and built the fake social media pages and their fake chat streams.\n\n![botarazzi image 1](../images/botarazzi/cityconcept-thumb.jpg)*above: concept art by me for bot-a-razzi.*\n![botarazzi image 1](../images/botarazzi/bot-a-razzi-long.png)*above: baby room (there are three rooms) in bot-a-razzi.*\n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/misc.md","__gatsby_resolved":{"frontmatter":{"path":"/other"}}},"previous":{"id":"385ae959-16b7-52ff-aa9f-8d3c6622fb96","children":[],"parent":"1ae790ce-de72-5d23-a990-8fda4b28f631","internal":{"content":"\n# About\nA Unity and Arduino version of popular game Cooking Mama! I made two felt fabric controllers (one spam, one steak) shaped like food and put sensors in them.\nWhen I first learned how to use an Arduino, one of my first instincts was to connect it with Unity. At first, I had trouble coming up with an idea for a unity and arduino experience that would leverage the advantages of both. Ultimately, I was inspired by Cooking Mama, a game that I had loved in my childhood. I was excited by the idea of enhancing the game experience with a physical controller. How will a felt knife and a felt steak compare to a Nintendo stylus? Will it make it more fun? Will it be the same?\n\nA demo vieo of this project was popular on Twitter and recieved over half a million views. [See the original tweet here.](https://twitter.com/crabbage_/status/1072711212016828416)\n\nThank you to Sidney Church for helping me connect the Arduino to unity and for all of the Arduino help!\n\n![example gif of me using the cooking mama felt controller. I am chopping a felt steak, which triggers changes in the game running on the laptop](../images/cooking-mama/meat-salad.gif)\n\n*gif of me using the cooking mama felt controller*\n\n<div class=\"br\"></div>\n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/a2O5I6Tr3ms\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</br>\n\n# Documentation pictures & Code\nThe look of the felt fabric food controllers was inspired by Lucy Sparrow's Sparrow Mart, a supermarket made up of food made from felt.\nYou can find the code [here](https://github.com/khanniie/unity-arduino-cooking-mama).\nThe steak controller contains six buttons attached to a breadboard. On top of the buttons, I laid flat acrylic boards that I cut into sections so that the buttons could have a greater range of influence. The spam contains a photocell that can tell when you remove the meat, and the bowl also contains a photocell to detect when the meat is placed inside.\nFor the unity part, I am sending the information from the Arduino to serial port, which unity then reads from.\n![diagram of how it works](../images/cooking-mama/meat-salad-diagram.jpg)\n*diagram of how it works* \n\n![images of the felt controller in progress](../images/cooking-mama/meat-salad-documentation.jpg)\n*left: the felt steak and fabric parts, right: developing with arduino*\n    \n","type":"MarkdownRemark","contentDigest":"6c596569570c4691bc73ce139ce002a1","counter":155,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"Cooking Mama Inspired Arduino controllers","path":"/cooking-game","date":"Fall 2018","coverImage":"../images/cooking_mama.jpg","coverVideo":"../assets/cooking_mama.mp4","excerpt":"Unity and Arduino version of popular game Cooking Mama. Made felt fabric controllers shaped like food and put sensors in them. Tweet received over half a million views on Twitter.","tags":["Unity","Physical computing","Arduino","Soft fabrication"],"links":"on Twitter,https://twitter.com/crabbage_/status/1072711212016828416","readMoreButton":"Learn more","order":7,"shortTitle":"Cooking Mama"},"excerpt":"","rawMarkdownBody":"\n# About\nA Unity and Arduino version of popular game Cooking Mama! I made two felt fabric controllers (one spam, one steak) shaped like food and put sensors in them.\nWhen I first learned how to use an Arduino, one of my first instincts was to connect it with Unity. At first, I had trouble coming up with an idea for a unity and arduino experience that would leverage the advantages of both. Ultimately, I was inspired by Cooking Mama, a game that I had loved in my childhood. I was excited by the idea of enhancing the game experience with a physical controller. How will a felt knife and a felt steak compare to a Nintendo stylus? Will it make it more fun? Will it be the same?\n\nA demo vieo of this project was popular on Twitter and recieved over half a million views. [See the original tweet here.](https://twitter.com/crabbage_/status/1072711212016828416)\n\nThank you to Sidney Church for helping me connect the Arduino to unity and for all of the Arduino help!\n\n![example gif of me using the cooking mama felt controller. I am chopping a felt steak, which triggers changes in the game running on the laptop](../images/cooking-mama/meat-salad.gif)\n\n*gif of me using the cooking mama felt controller*\n\n<div class=\"br\"></div>\n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/a2O5I6Tr3ms\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</br>\n\n# Documentation pictures & Code\nThe look of the felt fabric food controllers was inspired by Lucy Sparrow's Sparrow Mart, a supermarket made up of food made from felt.\nYou can find the code [here](https://github.com/khanniie/unity-arduino-cooking-mama).\nThe steak controller contains six buttons attached to a breadboard. On top of the buttons, I laid flat acrylic boards that I cut into sections so that the buttons could have a greater range of influence. The spam contains a photocell that can tell when you remove the meat, and the bowl also contains a photocell to detect when the meat is placed inside.\nFor the unity part, I am sending the information from the Arduino to serial port, which unity then reads from.\n![diagram of how it works](../images/cooking-mama/meat-salad-diagram.jpg)\n*diagram of how it works* \n\n![images of the felt controller in progress](../images/cooking-mama/meat-salad-documentation.jpg)\n*left: the felt steak and fabric parts, right: developing with arduino*\n    \n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/cooking_mama.md","__gatsby_resolved":{"frontmatter":{"path":"/cooking-game"}}}}},"staticQueryHashes":["236058478","3128451518"]}