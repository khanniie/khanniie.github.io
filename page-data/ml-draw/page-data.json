{"componentChunkName":"component---src-templates-page-js","path":"/ml-draw","result":{"data":{"markdownRemark":{"frontmatter":{"title":"mldraw Drawing Tool","date":"Spring 2019 - Spring 2020","path":"/ml-draw","author":"Elliot","excerpt":"Mldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.","tags":["Frontend","Typescript","ML","UI design"],"coverImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdqmyRh//8QAGhAAAQUBAAAAAAAAAAAAAAAAAQACERIgIf/aAAgBAQABBQJo5AVRj//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABkQAQEBAAMAAAAAAAAAAAAAAAEAERAhYf/aAAgBAQABPyEEucXnHUtt/9oADAMBAAIAAwAAABAvH//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAEDAQE/EKf/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPxCq/8QAHBAAAwEAAgMAAAAAAAAAAAAAAAERIUFRYaHh/9oACAEBAAE/EHQ07z5KNZ7LfY6SQQ0pmlds/9k=","aspectRatio":1.4814814814814814,"src":"/static/4b541ea4b47d2d1fa3f281a535e8079b/14b42/mldraw_thumb.jpg","srcSet":"/static/4b541ea4b47d2d1fa3f281a535e8079b/f836f/mldraw_thumb.jpg 200w,\n/static/4b541ea4b47d2d1fa3f281a535e8079b/2244e/mldraw_thumb.jpg 400w,\n/static/4b541ea4b47d2d1fa3f281a535e8079b/14b42/mldraw_thumb.jpg 800w,\n/static/4b541ea4b47d2d1fa3f281a535e8079b/a7715/mldraw_thumb.jpg 1000w","sizes":"(max-width: 800px) 100vw, 800px"}}},"coverVideo":{"publicURL":"/static/ba94c097b7cfff6562fda6092d4e6803/mldraw.mp4"},"links":""},"id":"5d3433e6-2945-58dc-97a9-725b0bbf7073","html":"<h1>About</h1>\n<p>Mldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.\nMldraw is a web app that uses a layered vector drawing system where each layer can be given a different machine learning model that translates user input. The user will give us a line drawing of edges, and our app's backend server renders the translation using whatever model is assigned to that layer.\nFurthermore, we've also made it easy to import custom ML models for researchers so that they can use our tool to experiment with their models!\nMade in collaboration with <a href=\"http://www.aman.work/\">Aman Tiwari</a>!\nI designed and built the UI using HTML/CSS/Typescript, and I also worked on the drawing and layer functionality, such as the\nvector drawing, drag and dropping objects, layer building, etc.\nMldraw’s interface is inspired by cute, techy/anti-techy retro aesthetics, such as the work of Sailor Mercury and the Bubblesort Zines.  We wanted it to be fun, novel, exciting and differentiated from the world of arxiv papers.\nMldraw was born out of seeing the potential of the body of research done using pix2pix to turn drawings into other images and the severe lack of a usable, “useful” and accessible tool to utilize this technology.</p>\n<h1>Videos</h1>\n<iframe class=\"iframe-560h\" src=\"https://player.vimeo.com/video/321893512\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<iframe class=\"iframe-620h\" src=\"https://player.vimeo.com/video/378759111\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<h1>Still in progress</h1>\n<p>We have a great demo, but have plans to bug-test, add more exciting ML models and features, and deploy at mldraw.com. We're also still experimenting and making big changes to the UI / UX! More documentation coming soon.\n<img src=\"../images/mldraw/mldraw_beforeafter.jpg\" alt=\"edges2pikachu example\"> <em>Left: User line drawing, Right: rendered version. Combined pikachu model and handbag model</em></p>\n<h1>Implementation</h1>\n<p>Mldraw is implemented as a Typescript frontend using choo.js as a UI framework, with a Python registry server and a Python adapter library, along with a number of instantiations of the adapter library for specific models.\nThe frontend communicates with the registry server using socket.io, which then passes to the frontend a list of models and their URLs. The frontend then communicates directly to the models.  This enables us e.g. to host a registry server for Mldraw without having to pay the cost of hosting every model it supports.\nMldraw also supports models that run locally on the client (in the above video, the cat, Pikachu and bag models run locally, while the other models are hosted on remote servers).\nIn service of the above desire to make Mldraw extensible, we have made it easy to add a new model – all that is required is some Python interface to the model, and to define a function that takes in an image and returns an image. Our model adapter will handle the rest of it, including registering the model with the server hosting a Mldraw interface.</p>\n<h1>Progress Documentation</h1>\n<video autoplay loop muted poster=\"../images/mldraw/mldraw_sequence.jpg\">\n    <source src=\"../images/mldraw/mldraw_sequence.mp4\">\n</video>\n<h2>CSS animations</h2>\n<p><img src=\"../images/mldraw/button-interactions.gif\" alt=\"render button interaction gif\"><em>Examples of small css animated interactions, Left: tooltip + pop animation on hover + lingering expansion until unhover, Middle: render button disappears when canvas in use, fades back in when done, Right: render button spins while rendering</em></p>\n<p><img src=\"../images/mldraw/toolbar.gif\" alt=\"toolbar interaction gif\"><em>toolbar animations: nod when usable, shake when unusable for that model</em></p>","excerpt":"About Mldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined…"}},"pageContext":{"type":"posts","next":{"id":"b137c10c-c88c-5886-81f1-c54d8278992c","children":[],"parent":"6eb1fc43-2c27-5914-8631-5c5b1607e9b3","internal":{"content":"\nA collaboration with Alice Fang and Sebastian Carpenter.\n\nZoöid is an exploration of light and color as a means for communication in a starless world. Inspired by deep-sea creatures that use light to communicate and coordinate, the looks in the line create an intricate lightshow on stage.\n\nThis project was funded with support by Carnegie Mellon’s Undergraduate Research Office, and the Frank-Ratchye Studio for Creative Inquiry. \n\n# Show video\n\n<iframe class=\"video-iframe\" src=\"https://player.vimeo.com/video/423457555\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Show images\n![zooid cover image](../images/zooid/zooid_1.JPG)\n![zooid cover image](../images/zooid/zooid_2.JPG)\n![zooid cover image](../images/zooid/zooid_3.JPG)\n![zooid cover image](../images/zooid/zooid_4.JPG)\n![zooid cover image](../images/zooid/zooid_end.JPG)\n\n# Concept\n\nInspired by Pyrosomes–deep sea colonies of small animals that use light to communicate and coordinate their actions–we are interested in exploring the ways light-based communication is used to synchronize actions; to achieve this, we want to create wearables that experiment with giving humans the same capabilities. Thus, our medium of dynamic light-based wearables is critical for our exploration, and our use of physical computation, including the development of a data-transfer protocol, is critical to achieving the level of fidelity necessary for our project.\n\n![zooid cover image](../images/zooid/moodboard.jpg)\n\nWe are hugely inspired by prior work by researchers who create fascinating cutting-edge wearables with real-life implications, such as Electrodermis by Morphing Matter Lab and Digits Nail Technologies. Visually, we are inspired by organic forms, like the work of Iris van Herpen.  However, although there are many instances of technologists who create specifically LED wearables, we have struggled to find many compelling examples of pieces that can work collectively. We are attempting to fill a gap in trends that we have observed— although physical computing is becoming more accessible, the use of LED lights in wearables remains at a one-dimensional, ornamental level, without meaningful integration. We want to explore LED wearables from a different angle, not using the lights as the only focus but rather as a medium in which we express an underlying networked system.\n\nWith clothing as part of a networked system that works collectively in communication with each other, instead of independent and isolated bodies, possibilities open up to the implications of ubiquitous technology, and the role it can play in future clothing design in the realm of performance, streetwear, disabilities resources and safety methods. For instance, we believe our system could potentially be adapted to benefit large groups working in dark or noisy environments, such as construction workers or firefighters: lights can be used to coordinate roles, and are capable of communicating across distance.\n\n\n# Process images\n\nFor the process of garment construction, base garments will be constructed with traditional sewing techniques, using tulle, spandex, cotton, and chiffon. The structural parts that house electronics will be created from 3D-printed flexible plastic that will allow us to print flat pieces that can conform to the model’s body. Our outfits use Arduinos and XBees, which allow each outfit to wirelessly communicate with a computer, which processes incoming data, such as music. Based on that data, color schemes and intensities are sent to XBee on each outfit, which receives the color scheme and intensity. We will develop our own protocol for data transfer in order to avoid the computer receiving multiple signals that interfere with each other. Colors change according to instructions received by the XBee, and the outfit will be lit with Adafruit NeoPixel LEDs. The lights will be dispersed through tulle fabric, fiber-optics, or acrylic plastic. \n\n![zooid cover image](../images/zooid/zooid-process3.jpg)\n![zooid cover image](../images/zooid/zooid-process4.jpg)\n![zooid cover image](../images/zooid/zooid-process5.jpg)\n![zooid cover image](../images/zooid/zooid-process6.jpg)\n\n","type":"MarkdownRemark","contentDigest":"d7fde487a4788f2f2adc4276fe94ec7b","counter":148,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"Zooïd Lunar Gala","date":"Fall 2019 - Spring 2020","path":"/zooid","author":"Lorem Ipsum","coverImage":"../images/zooid_thumb.jpg","coverVideo":"../assets/zooid_shadow.mp4","links":"","tags":["Physical computing","Soft fabrication"],"excerpt":"A line of eight light-art wearables, made for one of Pittsburgh's largest fashion shows.","readMoreButton":"See documentation","order":4,"shortTitle":"Zooid"},"excerpt":"","rawMarkdownBody":"\nA collaboration with Alice Fang and Sebastian Carpenter.\n\nZoöid is an exploration of light and color as a means for communication in a starless world. Inspired by deep-sea creatures that use light to communicate and coordinate, the looks in the line create an intricate lightshow on stage.\n\nThis project was funded with support by Carnegie Mellon’s Undergraduate Research Office, and the Frank-Ratchye Studio for Creative Inquiry. \n\n# Show video\n\n<iframe class=\"video-iframe\" src=\"https://player.vimeo.com/video/423457555\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Show images\n![zooid cover image](../images/zooid/zooid_1.JPG)\n![zooid cover image](../images/zooid/zooid_2.JPG)\n![zooid cover image](../images/zooid/zooid_3.JPG)\n![zooid cover image](../images/zooid/zooid_4.JPG)\n![zooid cover image](../images/zooid/zooid_end.JPG)\n\n# Concept\n\nInspired by Pyrosomes–deep sea colonies of small animals that use light to communicate and coordinate their actions–we are interested in exploring the ways light-based communication is used to synchronize actions; to achieve this, we want to create wearables that experiment with giving humans the same capabilities. Thus, our medium of dynamic light-based wearables is critical for our exploration, and our use of physical computation, including the development of a data-transfer protocol, is critical to achieving the level of fidelity necessary for our project.\n\n![zooid cover image](../images/zooid/moodboard.jpg)\n\nWe are hugely inspired by prior work by researchers who create fascinating cutting-edge wearables with real-life implications, such as Electrodermis by Morphing Matter Lab and Digits Nail Technologies. Visually, we are inspired by organic forms, like the work of Iris van Herpen.  However, although there are many instances of technologists who create specifically LED wearables, we have struggled to find many compelling examples of pieces that can work collectively. We are attempting to fill a gap in trends that we have observed— although physical computing is becoming more accessible, the use of LED lights in wearables remains at a one-dimensional, ornamental level, without meaningful integration. We want to explore LED wearables from a different angle, not using the lights as the only focus but rather as a medium in which we express an underlying networked system.\n\nWith clothing as part of a networked system that works collectively in communication with each other, instead of independent and isolated bodies, possibilities open up to the implications of ubiquitous technology, and the role it can play in future clothing design in the realm of performance, streetwear, disabilities resources and safety methods. For instance, we believe our system could potentially be adapted to benefit large groups working in dark or noisy environments, such as construction workers or firefighters: lights can be used to coordinate roles, and are capable of communicating across distance.\n\n\n# Process images\n\nFor the process of garment construction, base garments will be constructed with traditional sewing techniques, using tulle, spandex, cotton, and chiffon. The structural parts that house electronics will be created from 3D-printed flexible plastic that will allow us to print flat pieces that can conform to the model’s body. Our outfits use Arduinos and XBees, which allow each outfit to wirelessly communicate with a computer, which processes incoming data, such as music. Based on that data, color schemes and intensities are sent to XBee on each outfit, which receives the color scheme and intensity. We will develop our own protocol for data transfer in order to avoid the computer receiving multiple signals that interfere with each other. Colors change according to instructions received by the XBee, and the outfit will be lit with Adafruit NeoPixel LEDs. The lights will be dispersed through tulle fabric, fiber-optics, or acrylic plastic. \n\n![zooid cover image](../images/zooid/zooid-process3.jpg)\n![zooid cover image](../images/zooid/zooid-process4.jpg)\n![zooid cover image](../images/zooid/zooid-process5.jpg)\n![zooid cover image](../images/zooid/zooid-process6.jpg)\n\n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/zooid.md","__gatsby_resolved":{"frontmatter":{"path":"/zooid"}}},"previous":{"id":"5720aede-bfed-5d8a-aa99-5a265b75d988","children":[],"parent":"d2320169-eadb-51d6-9d6e-1abb08860b09","internal":{"content":"\n**Working with NASA JPL, we developed an interactive tool that enables scientists to compare data from their PIXL tool.**\n\nThis project was a collaboration between with Lukas Hermann, Nur Yildirim, and Shravya Bhat. \n\n# [Live URL](https://cmu-ids-2020.github.io/fp-airbnb-rats/)\n\n# [Github Repository](https://github.com/CMU-IDS-2020/fp-airbnb-rats)\n\n### Clustering server rate limitations\nWe are running the clustering backend on an AWS instance. Because it's a free trail of an instance, it will expire around Janurary 9th, 2021. *The app will continue to work afterwards, but the clustering feature will not work unless you set up your own server.* We are also using a free proxy server service (CORS-anywhere) to forward our CORS requests because otherwise we will get https client to http errors. **Due to the limits of free AWS and the free proxy server service, the rate and speed of responses may be limited if the app traffic is very high.**\n\nIn this case there are two possible solutions:\n1. Contact me and I can set up an alternative proxy server for you\n2. Clone our repository and set it up locally using the instructions in the README.\n\n![demo gif](../images/nasa/pixl.gif)\n\nDuring the project, I maintained and set up the components in our web app, which was built in React. I implemented the design of the interface, set up information coordination between components, and programmed the majority of the interactions in the interface, such as: \n- the lasso tool\n- adding and removing groups\n- the image viewer/overlay component with pan/zoom\n- group annotation and locking\n- element filtering and sorting\n- and more! \n\nI also worked with Lukas on the visualizations using d3.js. Lastly, I conceptualized and designed the core interactions, which I mocked up in Figma and validated with the key stakeholder, a NASA Co-Investigator, with a user study protocol that I created.\n\n# Abstract\n\nNASA JPL scientists working on the micro x-ray fluorescence (microXRF) spectroscopy data collected from Mars surface perform data analysis to look for signs of past microbial life on Mars. Their data analysis workflow mainly involves identifying mineral compounds through the element abundance in spatially distributed data points. Working with the NASA JPL team, we identified pain points and needs to further develop their existing data visualization and analysis tool. Specially, the team desired improvements for the process of creating and interpreting mineral composition groups. To address this problem, we developed an interactive tool that enables scientists to (1) semi-automatically cluster the data, and (2) compare the clusters and individual data points to make informed decisions about mineral compositions. Our tool supports a hybrid data analysis workflow where the user can manually refine the machine generated clusters. \n\n# Introduction\n\nOur project is carried out in collaboration with NASA Jet Propulsion Laboratory Human Interfaces Group in order to support the exploratory analysis of astrobiological data collected via Mars 2020 Perseverance rover. The mission of NASA scientists is to look for signs of past microbial life on Mars by examining the chemical makeup of rock and soil textures at a very fine scale. This is performed by analysing the micro x-ray fluorescence (microXRF) spectroscopy data collected by The Planetary Instrument for X-ray Lithochemistry (PIXL). By looking at the concentration and the spatial distribution of elements, the scientists are able to interpret the microXRF data and identify the mineral compounds that were likely created by microbes [2].\n\nThe PIXL instrument team performing this work include astrobiologists, sedimentologists, igneous petrologists, spectroscopists, and geochemists. An initial data visualisation tool prototype, Pixlise, is already in place to support scientists in this process [3]. Our preliminary discussions with the project team, therefore, focused on identifying the tasks and information needs of the scientists that might not be supported by the current app. In Pixlise, the data analysis tasks are mainly carried out using a map interface as the collected data involves spatially defined points with coordinates. The data analysis workflow has two major stages: (1) assigning elements to data points based on the spectroscopy data, (2) creating and interpreting mineral composition groups based on elemental abundance. Through a collaborative problem defining process, we decided to focus on the latter stage as the system in place is mostly targeted towards the former stage tasks.\n\nIn order to support the process of interpretation, we developed an interactive data visualization tool that **presents contributions in terms of the comparison and clustering of data**. Our standalone app presents an exploration of features and improvements that could be embedded into the future versions of the Pixlise tool. In the following sections, we first describe the existing tool and dataset, we then describe our tool development process and the interactive visualization techniques we propose. Finally, we walk through some use cases to illustrate the scenarios where our tool can support the data analysis process.\n\nThis application was developed over the course of about a month. We first had an initial meeting with one of our stakeholders, the project lead UX manager at NASA JPL where we gathered resources and learned about the possible problems that we could tackle. After more research into the datasets provided and some brainstorming about possible problem scopes, we presented initial mockups to the UX manager. Based on the feedback, we produced new mockups of 10 possible new interaction models or features and presented them to the co-investigator astrologists, who is a main beneficiary. Their input helped to validate our initial mockups, guide future design decisions and also helped us to prioritize which features to focus on. In the remaining weeks, we built the final application based on these findings.\n\n# Process mockups\nInitial sketches\n![process1](../images/nasa/process1.png)\nFirst pass at showing comparison\n![process2](../images/nasa/process2.png)\n10 possible interactions\n![process3](../images/nasa/process3.png)\n\n# Dataset, Pixlise App and The Data Analysis Workflow\n\nPIXL data set is a collection of spatially localized spectroscopy data as the instrument on Perseverance rover passes over the sampling area. The microXRF data has 1024 ordinal channels, each with a count of x-rays sensed by the instrument at that particular channel. The dataset is organized into a CSV file that contains information on points, coordinate data (X, Y, Z), and spectrum data with more than 4000 points per each data point.\n\nThe current data visualization tool facilitates the data analysis workflow through multiple interaction techniques (Figure 1). The main interaction touchpoint is the context image pane (Figure 1a) that displays the discrete points in the microXRF data. Using this map display allows scientists to visually isolate, select, and analyze discrete geological features within a sample data point [1]. As a first step, users have to assign elements to each data point. The spectrum pane (Figure 1b) visualizes the energy spectral density [5] of data points individually or in bulk. By looking at the peaks in the wavelength, users are able to infer element transitions. The instrument can detect over 20 elements, including Na, Mg, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Br, Rb, Sr, Y, Ga, Ge, As, Zr, Rb, Sr, Y and Zr at 10’s ppm level [2]. The element assignment process is semi-automatic as the tool can infer the presence of some elements, yet the domain knowledge of scientists are necessary to fine tune and validate the assignment.\n\n![report image 2](../images/nasa/report_img_2.png)\n\n*Figure 1: Pixlise Interface*\n\nOnce the elements are assigned, users introspect the element abundance to make associations for mineral identification using supporting panes. The histogram tool (Figure 1c) shows the quantified weight percentages of different elements in selected samples in linear or log scale. The chord diagram (Figure 1d), the ternary diagram (Figure 1e), and the binary diagram (Figure 1f) indicate the correlations between abundant elements to inform the interpretation of mineral compositions.\n\nWhile the Pixlise tool supports the process of element assignment and quantification, there is little support in terms of comparison between points and regions. Our interviews with the stakeholders, including the project lead UX Manager and the co-lead research scientist, revealed that there are several pain points and needs in terms of selection, grouping, annotation, clustering, and comparison. Specifically, the current interface does not provide any means to do multiple selections (individual or group) at the same time in order to compare the element distributions across different regions. Additionally, the scientists desired to have auto generated clusters with various algorithm options, so that they do not have to start from scratch and instead semi-automatically create groups of the mapped area. Our tool aims to address these gaps through extending the existing data analysis workflow with novel and targeted interaction design solutions.\n\n# Pixlise+\n\nWe present an interactive data visualization tool, Pixlise+, that extends the current system in place. We built our tool as a standalone application instead of integrated development, as the current architecture has several authentication layers that makes the development process difficult. However, our tool builds on the current application in terms of interface layout, interaction styles, and overall design to maintain familiarity.\n\n\n![report image 1](../images/nasa/report_img_1.jpg)\n\n*Figure 2: Pixlise+ Interface*\n\n*Pixlise+ interface consists of four main components: Context Image (Figure 2a), Comparison Sandbox (Figure 2b), Clustering (Figure 2c), and Parallel Coordinate Plots (Figure 2d). Below, we describe each component in detail.*\n\n**Context Image:** This 2D map is the main interaction touchpoint for the application. The visualization plots the collected data on the geographical context image as dot per point. There are 6400 data points arranged into a grid based on their spatial coordinates. Overlaying the datapoint over the real terrain view allows scientists to select and analyze data in accordance with the context. There are three major actions the users can take in this view. First, using the navigation icon, they can pan and zoom (in or out). Second, using the examining tool (eye icon), they can hover over individual points to view related data in isolation, which is displayed in the Comparison Sandbox. Finally, the scientists can select and group the data points using the lasso tool and component dropdown menu. Once a component group is created by clicking on “add item”, the selected data points will be displayed in that group’s color. Consequent lasso actions will add to the selection, while holding down shift will remove points from the selection. Detailed information about the groups in terms of element concentration is displayed in the Comparison Sandbox, so that the scientists can iterate on grouping data into clusters based on similarity. \n\n**Comparison Sandbox:** This view displays the element percentages in the selected data points as histograms. As opposed to displaying one histogram at a time as in the previous implementation, our tool can display up to 20 histograms for the comparison of potential mineral compositions across clusters. When initiated the Comparison Sandbox displays the average element weights of all samples in the dataset in logarithmic or linear scale. The element symbols are displayed as columns at the top and users are able to hide less important elements to focus on the most relevant ones. The histogram can be sorted based on the element abundance (using means, high to low or low to high) or the coefficient of variation (standard deviation divided by the mean). The histogram tooltip displays more detailed statistical information per element, including mean, standard deviation, min, max, Q1, Q3, and the median. This information is useful to help the user determine how the distribution of data looks. Min, max, Q1, Q3 and median are especially important if the distribution is not a normal distribution. Additionally, variance in an element is shown as an inner bar within each bar of the histogram, which informs whether selected data points demonstrate similar characteristics.\n\nAs users create multiple groups using the selection and component creation tools in the Context Image view, the histograms in the Comparison Sandbox update to display multiple groups at once. Using histogram controls, users can lock and unlock a group before finalizing their selection. Additionally, they can annotate the group with insights. Once a group is annotated, hovering over the group in the Context Image view shows the annotations overlaid on top. Finally, when users want to examine how a single data point fits in a group, hovering over the data point in the Context Image view displays the element percentages of the individual data point across all histograms as overlaid ticks on bars.\n\n**Parallel Coordinates Plots:** Histograms are useful in characterizing groups through element abundance, but it can be difficult to distinguish fine grain differences among the data. To enable a more granular exploration, Parallel Coordinates Plots (PCP) displays percent values based on the minimum and maximum range. Therefore, nuanced differences between groups can be seen per each element.\n\n**Clustering:** Grouping the data points into clusters are usually carried out manually as the scientists have extensive expertise in terms of identifying geologic features in images. However, machine learning algorithms can provide particular or generalizable insights depending on the dataset. Our tool utilizes various ML techniques for auto generating clusters as starting points for a semi-automatic data analysis workflow. Clustering tool provides three different algorithms: k-means clustering, hierarchical clustering, and minimum maximum. K-means clustering partitions the dataset into a user defined number of clusters using the distance between the points. Scientists can choose to apply dimensionality reduction using PCA or t-SNE. PCA performs deterministic linear dimensionality reduction and takes in variance as a hyperparameter. T-SNE, on the other hand, performs probabilistic and non-linear dimensionality reduction and takes in perplexity as a hyperparameter.\n\nHierarchical clustering builds a hierarchy of clusters based on a distance metric defined by the linkage hyperparameters (ward, complete, average, or single). Similar to k-means, the number of clusters is user defined in hierarchical clustering. Finally, users can create auto generated groups without using a clustering algorithm. In this case, they can group the dataset based on maximum or minimum abundance of elements. The clustering tool aims to enable scientists to play around with different clustering options and fine tune parameters for finding insights.\n\n# Technical Implementation\n\nWe developed our application using React and deployed it using Github pages. Our code is open source [4]. While React was used to develop the UI and coordinate the different components across the application, D3.js, a popular visualization Javascript library, was used to create the visualizations within the components. \n\nFor the clustering implementation, we use a Python backend to dynamically generate cluster information based on the many different hyperparameters that our app allows.The client app and the Python backend communicate using Flask and an ajax call using jQuery. The backend itself is hosted on Amazon Web Services. The clustering backend uses popular data science libraries such as sklearn and pandas. Sklearn is used to perform all of our machine learning algorithms, including hierarchical clustering, t-SNE, PCA, and k-means. Our application also enables hyperparameters such as linkage, variance percentage, perplexity, and number of clusters, to be passed to the backend as well.\n\n# Results\n\nWe present a walkthrough of our tool in our project demonstration video using a sample dataset called “King’s Court”. This dataset has samples from a terrain with a crater-like geological feature. Using the clustering tool, we were able to test various algorithms and hyperparameters for grouping the dataset into meaningful subsets. Our explorative results show that both PCA and t-SNE were able to identify the large cluster around the crater (Figure 2a-1) with multiple discrete groups around its perimeters (Figure 2a-2). Once the initial groups were identified using the clustering algorithm, we used the hovering feature in the Context Image tools for examining individual data points within clusters. Based on the histogram information in the Comparison Sandbox, we were able to add or remove specific data points using the lasso tool.\n\n# Discussion and Future Work\n\nThe main contributions of our work include interaction techniques for the comparative analysis and clustering of microXRF spectroscopy data in the context of the PIXL project. Our tool Pixlise+ enabled a semi-automatic data analysis workflow that enabled an augmented grouping of microXRF data based on element abundance. Using built-in statistical analyses, scientists can compare several clusters at once to fine tune and validate the grouping of the data. Moreover, tools that support documentation including locking and annotation open the way to future developments for multi-user collaboration.\n\nOur work reveals several directions for future research. For the overall data analysis workflow, blending human and machine intelligence raises major research questions. How can we combine manually created groups with machine generated clusters to provide support scientists’ agency while removing tedious work? What kind of interaction techniques (e.g. overlays) might be suited for representing human and machine generated groups? Should the system suggest potential clusters to validate human insights through mixed initiative interaction? These are some questions we would like to explore going forward. In addition, sharing annotated datasets present an interesting direction for future research in terms of collaborative and asynchronous workflows. Potential techniques include exporting screenshots, sharing workspaces, and sharing the dataset to be opened with a different program or tool. We will also explore the database requirements to allow saving and sharing.\n\nFinally, our short term goals include the integration of Pixlise+ into the current system used by NASA JPL. We will continue working with our collaborators to collect feedback on our tool through user testing, iterative development, and deployment.\n\n# Special thanks\n\nThank you to IDS professors Dominic Mortiz and Adam Perer and the TAs for their help!\n\n**References**\n\n[1] PIXELATE: novel visualization and computational methods for the analysis of astrobiological spectroscopy data. [https://www.researchgate.net/publication/338924853_PIXELATE_novel_visualization_and_computational_methods_for_the_analysis_of_astrobiological_spectroscopy_data](https://www.researchgate.net/publication/338924853_PIXELATE_novel_visualization_and_computational_methods_for_the_analysis_of_astrobiological_spectroscopy_data)\n\n[2] PIXL for Scientists. [https://mars.nasa.gov/mars2020/spacecraft/instruments/pixl/for-scientists/](https://mars.nasa.gov/mars2020/spacecraft/instruments/pixl/for-scientists/)\n\n[3] PIXLISE Mars Rock Sample Investigation. [https://datavis.caltech.edu/projects/pixl/](https://datavis.caltech.edu/projects/pixl/)\n\n[4] Pixlise+ Github Repository. [https://github.com/CMU-IDS-2020/fp-airbnb-rats](https://github.com/CMU-IDS-2020/fp-airbnb-rats)\n\n[5] Spectral Density. [https://en.wikipedia.org/wiki/Spectral_density](https://en.wikipedia.org/wiki/Spectral_density)","type":"MarkdownRemark","contentDigest":"42c0297657b0bfa6e3ef30b68e3a61b9","counter":151,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"NASA JPL: Pixelise+","path":"/nasa","date":"Fall 2020","coverImage":"../images/nasa.jpg","coverVideo":"../assets/nasa.mp4","author":"Elliot","excerpt":"Working with NASA JPL, we developed an interactive tool that enables scientists to compare data from their PIXL tool.","tags":["Frontend","Data vis","React","d3.js","ML"],"links":"","readMoreButton":"See documentation","order":2,"shortTitle":"Pixelise+"},"excerpt":"","rawMarkdownBody":"\n**Working with NASA JPL, we developed an interactive tool that enables scientists to compare data from their PIXL tool.**\n\nThis project was a collaboration between with Lukas Hermann, Nur Yildirim, and Shravya Bhat. \n\n# [Live URL](https://cmu-ids-2020.github.io/fp-airbnb-rats/)\n\n# [Github Repository](https://github.com/CMU-IDS-2020/fp-airbnb-rats)\n\n### Clustering server rate limitations\nWe are running the clustering backend on an AWS instance. Because it's a free trail of an instance, it will expire around Janurary 9th, 2021. *The app will continue to work afterwards, but the clustering feature will not work unless you set up your own server.* We are also using a free proxy server service (CORS-anywhere) to forward our CORS requests because otherwise we will get https client to http errors. **Due to the limits of free AWS and the free proxy server service, the rate and speed of responses may be limited if the app traffic is very high.**\n\nIn this case there are two possible solutions:\n1. Contact me and I can set up an alternative proxy server for you\n2. Clone our repository and set it up locally using the instructions in the README.\n\n![demo gif](../images/nasa/pixl.gif)\n\nDuring the project, I maintained and set up the components in our web app, which was built in React. I implemented the design of the interface, set up information coordination between components, and programmed the majority of the interactions in the interface, such as: \n- the lasso tool\n- adding and removing groups\n- the image viewer/overlay component with pan/zoom\n- group annotation and locking\n- element filtering and sorting\n- and more! \n\nI also worked with Lukas on the visualizations using d3.js. Lastly, I conceptualized and designed the core interactions, which I mocked up in Figma and validated with the key stakeholder, a NASA Co-Investigator, with a user study protocol that I created.\n\n# Abstract\n\nNASA JPL scientists working on the micro x-ray fluorescence (microXRF) spectroscopy data collected from Mars surface perform data analysis to look for signs of past microbial life on Mars. Their data analysis workflow mainly involves identifying mineral compounds through the element abundance in spatially distributed data points. Working with the NASA JPL team, we identified pain points and needs to further develop their existing data visualization and analysis tool. Specially, the team desired improvements for the process of creating and interpreting mineral composition groups. To address this problem, we developed an interactive tool that enables scientists to (1) semi-automatically cluster the data, and (2) compare the clusters and individual data points to make informed decisions about mineral compositions. Our tool supports a hybrid data analysis workflow where the user can manually refine the machine generated clusters. \n\n# Introduction\n\nOur project is carried out in collaboration with NASA Jet Propulsion Laboratory Human Interfaces Group in order to support the exploratory analysis of astrobiological data collected via Mars 2020 Perseverance rover. The mission of NASA scientists is to look for signs of past microbial life on Mars by examining the chemical makeup of rock and soil textures at a very fine scale. This is performed by analysing the micro x-ray fluorescence (microXRF) spectroscopy data collected by The Planetary Instrument for X-ray Lithochemistry (PIXL). By looking at the concentration and the spatial distribution of elements, the scientists are able to interpret the microXRF data and identify the mineral compounds that were likely created by microbes [2].\n\nThe PIXL instrument team performing this work include astrobiologists, sedimentologists, igneous petrologists, spectroscopists, and geochemists. An initial data visualisation tool prototype, Pixlise, is already in place to support scientists in this process [3]. Our preliminary discussions with the project team, therefore, focused on identifying the tasks and information needs of the scientists that might not be supported by the current app. In Pixlise, the data analysis tasks are mainly carried out using a map interface as the collected data involves spatially defined points with coordinates. The data analysis workflow has two major stages: (1) assigning elements to data points based on the spectroscopy data, (2) creating and interpreting mineral composition groups based on elemental abundance. Through a collaborative problem defining process, we decided to focus on the latter stage as the system in place is mostly targeted towards the former stage tasks.\n\nIn order to support the process of interpretation, we developed an interactive data visualization tool that **presents contributions in terms of the comparison and clustering of data**. Our standalone app presents an exploration of features and improvements that could be embedded into the future versions of the Pixlise tool. In the following sections, we first describe the existing tool and dataset, we then describe our tool development process and the interactive visualization techniques we propose. Finally, we walk through some use cases to illustrate the scenarios where our tool can support the data analysis process.\n\nThis application was developed over the course of about a month. We first had an initial meeting with one of our stakeholders, the project lead UX manager at NASA JPL where we gathered resources and learned about the possible problems that we could tackle. After more research into the datasets provided and some brainstorming about possible problem scopes, we presented initial mockups to the UX manager. Based on the feedback, we produced new mockups of 10 possible new interaction models or features and presented them to the co-investigator astrologists, who is a main beneficiary. Their input helped to validate our initial mockups, guide future design decisions and also helped us to prioritize which features to focus on. In the remaining weeks, we built the final application based on these findings.\n\n# Process mockups\nInitial sketches\n![process1](../images/nasa/process1.png)\nFirst pass at showing comparison\n![process2](../images/nasa/process2.png)\n10 possible interactions\n![process3](../images/nasa/process3.png)\n\n# Dataset, Pixlise App and The Data Analysis Workflow\n\nPIXL data set is a collection of spatially localized spectroscopy data as the instrument on Perseverance rover passes over the sampling area. The microXRF data has 1024 ordinal channels, each with a count of x-rays sensed by the instrument at that particular channel. The dataset is organized into a CSV file that contains information on points, coordinate data (X, Y, Z), and spectrum data with more than 4000 points per each data point.\n\nThe current data visualization tool facilitates the data analysis workflow through multiple interaction techniques (Figure 1). The main interaction touchpoint is the context image pane (Figure 1a) that displays the discrete points in the microXRF data. Using this map display allows scientists to visually isolate, select, and analyze discrete geological features within a sample data point [1]. As a first step, users have to assign elements to each data point. The spectrum pane (Figure 1b) visualizes the energy spectral density [5] of data points individually or in bulk. By looking at the peaks in the wavelength, users are able to infer element transitions. The instrument can detect over 20 elements, including Na, Mg, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Br, Rb, Sr, Y, Ga, Ge, As, Zr, Rb, Sr, Y and Zr at 10’s ppm level [2]. The element assignment process is semi-automatic as the tool can infer the presence of some elements, yet the domain knowledge of scientists are necessary to fine tune and validate the assignment.\n\n![report image 2](../images/nasa/report_img_2.png)\n\n*Figure 1: Pixlise Interface*\n\nOnce the elements are assigned, users introspect the element abundance to make associations for mineral identification using supporting panes. The histogram tool (Figure 1c) shows the quantified weight percentages of different elements in selected samples in linear or log scale. The chord diagram (Figure 1d), the ternary diagram (Figure 1e), and the binary diagram (Figure 1f) indicate the correlations between abundant elements to inform the interpretation of mineral compositions.\n\nWhile the Pixlise tool supports the process of element assignment and quantification, there is little support in terms of comparison between points and regions. Our interviews with the stakeholders, including the project lead UX Manager and the co-lead research scientist, revealed that there are several pain points and needs in terms of selection, grouping, annotation, clustering, and comparison. Specifically, the current interface does not provide any means to do multiple selections (individual or group) at the same time in order to compare the element distributions across different regions. Additionally, the scientists desired to have auto generated clusters with various algorithm options, so that they do not have to start from scratch and instead semi-automatically create groups of the mapped area. Our tool aims to address these gaps through extending the existing data analysis workflow with novel and targeted interaction design solutions.\n\n# Pixlise+\n\nWe present an interactive data visualization tool, Pixlise+, that extends the current system in place. We built our tool as a standalone application instead of integrated development, as the current architecture has several authentication layers that makes the development process difficult. However, our tool builds on the current application in terms of interface layout, interaction styles, and overall design to maintain familiarity.\n\n\n![report image 1](../images/nasa/report_img_1.jpg)\n\n*Figure 2: Pixlise+ Interface*\n\n*Pixlise+ interface consists of four main components: Context Image (Figure 2a), Comparison Sandbox (Figure 2b), Clustering (Figure 2c), and Parallel Coordinate Plots (Figure 2d). Below, we describe each component in detail.*\n\n**Context Image:** This 2D map is the main interaction touchpoint for the application. The visualization plots the collected data on the geographical context image as dot per point. There are 6400 data points arranged into a grid based on their spatial coordinates. Overlaying the datapoint over the real terrain view allows scientists to select and analyze data in accordance with the context. There are three major actions the users can take in this view. First, using the navigation icon, they can pan and zoom (in or out). Second, using the examining tool (eye icon), they can hover over individual points to view related data in isolation, which is displayed in the Comparison Sandbox. Finally, the scientists can select and group the data points using the lasso tool and component dropdown menu. Once a component group is created by clicking on “add item”, the selected data points will be displayed in that group’s color. Consequent lasso actions will add to the selection, while holding down shift will remove points from the selection. Detailed information about the groups in terms of element concentration is displayed in the Comparison Sandbox, so that the scientists can iterate on grouping data into clusters based on similarity. \n\n**Comparison Sandbox:** This view displays the element percentages in the selected data points as histograms. As opposed to displaying one histogram at a time as in the previous implementation, our tool can display up to 20 histograms for the comparison of potential mineral compositions across clusters. When initiated the Comparison Sandbox displays the average element weights of all samples in the dataset in logarithmic or linear scale. The element symbols are displayed as columns at the top and users are able to hide less important elements to focus on the most relevant ones. The histogram can be sorted based on the element abundance (using means, high to low or low to high) or the coefficient of variation (standard deviation divided by the mean). The histogram tooltip displays more detailed statistical information per element, including mean, standard deviation, min, max, Q1, Q3, and the median. This information is useful to help the user determine how the distribution of data looks. Min, max, Q1, Q3 and median are especially important if the distribution is not a normal distribution. Additionally, variance in an element is shown as an inner bar within each bar of the histogram, which informs whether selected data points demonstrate similar characteristics.\n\nAs users create multiple groups using the selection and component creation tools in the Context Image view, the histograms in the Comparison Sandbox update to display multiple groups at once. Using histogram controls, users can lock and unlock a group before finalizing their selection. Additionally, they can annotate the group with insights. Once a group is annotated, hovering over the group in the Context Image view shows the annotations overlaid on top. Finally, when users want to examine how a single data point fits in a group, hovering over the data point in the Context Image view displays the element percentages of the individual data point across all histograms as overlaid ticks on bars.\n\n**Parallel Coordinates Plots:** Histograms are useful in characterizing groups through element abundance, but it can be difficult to distinguish fine grain differences among the data. To enable a more granular exploration, Parallel Coordinates Plots (PCP) displays percent values based on the minimum and maximum range. Therefore, nuanced differences between groups can be seen per each element.\n\n**Clustering:** Grouping the data points into clusters are usually carried out manually as the scientists have extensive expertise in terms of identifying geologic features in images. However, machine learning algorithms can provide particular or generalizable insights depending on the dataset. Our tool utilizes various ML techniques for auto generating clusters as starting points for a semi-automatic data analysis workflow. Clustering tool provides three different algorithms: k-means clustering, hierarchical clustering, and minimum maximum. K-means clustering partitions the dataset into a user defined number of clusters using the distance between the points. Scientists can choose to apply dimensionality reduction using PCA or t-SNE. PCA performs deterministic linear dimensionality reduction and takes in variance as a hyperparameter. T-SNE, on the other hand, performs probabilistic and non-linear dimensionality reduction and takes in perplexity as a hyperparameter.\n\nHierarchical clustering builds a hierarchy of clusters based on a distance metric defined by the linkage hyperparameters (ward, complete, average, or single). Similar to k-means, the number of clusters is user defined in hierarchical clustering. Finally, users can create auto generated groups without using a clustering algorithm. In this case, they can group the dataset based on maximum or minimum abundance of elements. The clustering tool aims to enable scientists to play around with different clustering options and fine tune parameters for finding insights.\n\n# Technical Implementation\n\nWe developed our application using React and deployed it using Github pages. Our code is open source [4]. While React was used to develop the UI and coordinate the different components across the application, D3.js, a popular visualization Javascript library, was used to create the visualizations within the components. \n\nFor the clustering implementation, we use a Python backend to dynamically generate cluster information based on the many different hyperparameters that our app allows.The client app and the Python backend communicate using Flask and an ajax call using jQuery. The backend itself is hosted on Amazon Web Services. The clustering backend uses popular data science libraries such as sklearn and pandas. Sklearn is used to perform all of our machine learning algorithms, including hierarchical clustering, t-SNE, PCA, and k-means. Our application also enables hyperparameters such as linkage, variance percentage, perplexity, and number of clusters, to be passed to the backend as well.\n\n# Results\n\nWe present a walkthrough of our tool in our project demonstration video using a sample dataset called “King’s Court”. This dataset has samples from a terrain with a crater-like geological feature. Using the clustering tool, we were able to test various algorithms and hyperparameters for grouping the dataset into meaningful subsets. Our explorative results show that both PCA and t-SNE were able to identify the large cluster around the crater (Figure 2a-1) with multiple discrete groups around its perimeters (Figure 2a-2). Once the initial groups were identified using the clustering algorithm, we used the hovering feature in the Context Image tools for examining individual data points within clusters. Based on the histogram information in the Comparison Sandbox, we were able to add or remove specific data points using the lasso tool.\n\n# Discussion and Future Work\n\nThe main contributions of our work include interaction techniques for the comparative analysis and clustering of microXRF spectroscopy data in the context of the PIXL project. Our tool Pixlise+ enabled a semi-automatic data analysis workflow that enabled an augmented grouping of microXRF data based on element abundance. Using built-in statistical analyses, scientists can compare several clusters at once to fine tune and validate the grouping of the data. Moreover, tools that support documentation including locking and annotation open the way to future developments for multi-user collaboration.\n\nOur work reveals several directions for future research. For the overall data analysis workflow, blending human and machine intelligence raises major research questions. How can we combine manually created groups with machine generated clusters to provide support scientists’ agency while removing tedious work? What kind of interaction techniques (e.g. overlays) might be suited for representing human and machine generated groups? Should the system suggest potential clusters to validate human insights through mixed initiative interaction? These are some questions we would like to explore going forward. In addition, sharing annotated datasets present an interesting direction for future research in terms of collaborative and asynchronous workflows. Potential techniques include exporting screenshots, sharing workspaces, and sharing the dataset to be opened with a different program or tool. We will also explore the database requirements to allow saving and sharing.\n\nFinally, our short term goals include the integration of Pixlise+ into the current system used by NASA JPL. We will continue working with our collaborators to collect feedback on our tool through user testing, iterative development, and deployment.\n\n# Special thanks\n\nThank you to IDS professors Dominic Mortiz and Adam Perer and the TAs for their help!\n\n**References**\n\n[1] PIXELATE: novel visualization and computational methods for the analysis of astrobiological spectroscopy data. [https://www.researchgate.net/publication/338924853_PIXELATE_novel_visualization_and_computational_methods_for_the_analysis_of_astrobiological_spectroscopy_data](https://www.researchgate.net/publication/338924853_PIXELATE_novel_visualization_and_computational_methods_for_the_analysis_of_astrobiological_spectroscopy_data)\n\n[2] PIXL for Scientists. [https://mars.nasa.gov/mars2020/spacecraft/instruments/pixl/for-scientists/](https://mars.nasa.gov/mars2020/spacecraft/instruments/pixl/for-scientists/)\n\n[3] PIXLISE Mars Rock Sample Investigation. [https://datavis.caltech.edu/projects/pixl/](https://datavis.caltech.edu/projects/pixl/)\n\n[4] Pixlise+ Github Repository. [https://github.com/CMU-IDS-2020/fp-airbnb-rats](https://github.com/CMU-IDS-2020/fp-airbnb-rats)\n\n[5] Spectral Density. [https://en.wikipedia.org/wiki/Spectral_density](https://en.wikipedia.org/wiki/Spectral_density)","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/nasa.md","__gatsby_resolved":{"frontmatter":{"path":"/nasa"}}}}},"staticQueryHashes":["236058478","3128451518"]}