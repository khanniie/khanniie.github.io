{"componentChunkName":"component---src-templates-page-js","path":"/google-ar","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Google AR Internship","date":"Summer/Fall 2020 (@WFH)","path":"/google-ar","author":"Lorem Ipsum","excerpt":"Developed Android apps to research cutting-edge user experiences within Google's Devices product area.","tags":null,"coverImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAIBBAX/xAAXAQEBAQEAAAAAAAAAAAAAAAAAAgED/9oADAMBAAIQAxAAAAH3ZiJ59SVXWBob/8QAGhABAAIDAQAAAAAAAAAAAAAAAQIRAAMQMf/aAAgBAQABBQJawlfNguFxI+d//8QAFhEBAQEAAAAAAAAAAAAAAAAAEQEg/9oACAEDAQE/AWuP/8QAFhEBAQEAAAAAAAAAAAAAAAAAEQEg/9oACAECAQE/AWuP/8QAFxABAAMAAAAAAAAAAAAAAAAAIAAhMf/aAAgBAQAGPwIZLH//xAAaEAEAAgMBAAAAAAAAAAAAAAABABEQMUEh/9oACAEBAAE/ITNsPS4eyoo1OyV5LJpTgx//2gAMAwEAAgADAAAAEI/Y/P/EABkRAAEFAAAAAAAAAAAAAAAAAAEAECExQf/aAAgBAwEBPxAxcQp//8QAGBEAAwEBAAAAAAAAAAAAAAAAARAhETH/2gAIAQIBAT8QEORDi//EABwQAQACAgMBAAAAAAAAAAAAAAEAESFREDFB8f/aAAgBAQABPxBR9IuPZikipcVV2V7lqmTGkbWwIlkJn3j/2Q==","aspectRatio":1,"src":"/static/698ba58914d5452c611ce0e24c0c3786/0075a/ar-thumb.jpg","srcSet":"/static/698ba58914d5452c611ce0e24c0c3786/f836f/ar-thumb.jpg 200w,\n/static/698ba58914d5452c611ce0e24c0c3786/2244e/ar-thumb.jpg 400w,\n/static/698ba58914d5452c611ce0e24c0c3786/0075a/ar-thumb.jpg 590w","sizes":"(max-width: 590px) 100vw, 590px"}}},"coverVideo":{"publicURL":"/static/20ac89a4e284b3b9e88cd66da9d35287/ar-thumbnail.mp4"},"links":""},"id":"3d8b1d1f-bc63-52ac-9b1d-6d5495cef785","html":"<h1>Project</h1>\n<p>In Summer and Fall 2020, I was a UX Engineering intern on Google's AR team (formerly known as Daydream). Although the nature of the project is still confidential, I can still talk generally about some of the skills that I used and the lessons that I learned. If you'd like to learn more, please reach out :-). If you're a Google employee, I can put you in touch with my intern host who may be able to provide more information.</p>\n<p>Towards the end of my summer internship, my team extended my internship through a small program at Google that allows interns to continue working part-time while in university. So during the Fall semester of my senior year, I worked part-time (20% for a month, and then 50% for two months).</p>\n<p>This was my first time working in a product-focused team. As part of the UX team, I worked closely with the other members of the UX team, but also interacted with PMs and software engineers. </p>\n<h1>Technologies / skills</h1>\n<ol>\n<li>Android app development with Android Studio</li>\n<li>Android/iOS app developemnt with Unity and AR Foundation</li>\n<li>Google Cloud Vision API integration</li>\n<li>Web development </li>\n<li>3D modeling with Autodesk Maya</li>\n<li>Demo and concept video editing with Adobe suite</li>\n<li>Illustration under the direction of a creative director for internal projects</li>\n<li>Human-centered design thinking</li>\n<li>Prototyping and iteration</li>\n<li>System and Input UX design</li>\n</ol>\n<p><img src=\"../images/google-ar/WFH.jpg\" alt=\"work from home\">\n<em>My desk while I worked from home!</em></p>","excerpt":"Project In Summer and Fall 2020, I was a UX Engineering intern on Google's AR team (formerly known as Daydream). Although the nature of the…"}},"pageContext":{"type":"posts","next":{"id":"5d3433e6-2945-58dc-97a9-725b0bbf7073","children":[],"parent":"f5f5b024-afb4-558a-9cb5-54bcf0d9a3b4","internal":{"content":"# About\nMldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.\nMldraw is a web app that uses a layered vector drawing system where each layer can be given a different machine learning model that translates user input. The user will give us a line drawing of edges, and our app's backend server renders the translation using whatever model is assigned to that layer.\nFurthermore, we've also made it easy to import custom ML models for researchers so that they can use our tool to experiment with their models!\nMade in collaboration with [Aman Tiwari](http://www.aman.work/)!\nI designed and built the UI using HTML/CSS/Typescript, and I also worked on the drawing and layer functionality, such as the\nvector drawing, drag and dropping objects, layer building, etc.\nMldraw’s interface is inspired by cute, techy/anti-techy retro aesthetics, such as the work of Sailor Mercury and the Bubblesort Zines.  We wanted it to be fun, novel, exciting and differentiated from the world of arxiv papers.\nMldraw was born out of seeing the potential of the body of research done using pix2pix to turn drawings into other images and the severe lack of a usable, “useful” and accessible tool to utilize this technology.\n\n# Videos\n\n<iframe class=\"iframe-560h\" src=\"https://player.vimeo.com/video/321893512\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<iframe class=\"iframe-620h\" src=\"https://player.vimeo.com/video/378759111\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Still in progress\nWe have a great demo, but have plans to bug-test, add more exciting ML models and features, and deploy at mldraw.com. We're also still experimenting and making big changes to the UI / UX! More documentation coming soon.\n![edges2pikachu example](../images/mldraw/mldraw_beforeafter.jpg) *Left: User line drawing, Right: rendered version. Combined pikachu model and handbag model*\n\n# Implementation\nMldraw is implemented as a Typescript frontend using choo.js as a UI framework, with a Python registry server and a Python adapter library, along with a number of instantiations of the adapter library for specific models.\nThe frontend communicates with the registry server using socket.io, which then passes to the frontend a list of models and their URLs. The frontend then communicates directly to the models.  This enables us e.g. to host a registry server for Mldraw without having to pay the cost of hosting every model it supports.\nMldraw also supports models that run locally on the client (in the above video, the cat, Pikachu and bag models run locally, while the other models are hosted on remote servers).\nIn service of the above desire to make Mldraw extensible, we have made it easy to add a new model – all that is required is some Python interface to the model, and to define a function that takes in an image and returns an image. Our model adapter will handle the rest of it, including registering the model with the server hosting a Mldraw interface.\n\n# Progress Documentation\n\n<video autoplay loop muted poster=\"../images/mldraw/mldraw_sequence.jpg\">\n    <source src=\"../images/mldraw/mldraw_sequence.mp4\">\n</video>\n\n## CSS animations\n![render button interaction gif](../images/mldraw/button-interactions.gif)*Examples of small css animated interactions, Left: tooltip + pop animation on hover + lingering expansion until unhover, Middle: render button disappears when canvas in use, fades back in when done, Right: render button spins while rendering*\n\n\n\n![toolbar interaction gif](../images/mldraw/toolbar.gif)*toolbar animations: nod when usable, shake when unusable for that model*\n","type":"MarkdownRemark","contentDigest":"03c8c313bd814c84d378f94e44af9362","counter":128,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"mldraw Drawing Tool","path":"/ml-draw","date":"Spring 2019 - Spring 2020","coverImage":"../images/mldraw_thumb.jpg","coverVideo":"../assets/mldraw.mp4","author":"Elliot","excerpt":"Mldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.","tags":["rob____ot","hello friend"],"links":"","readMoreButton":"See documentation","order":2},"excerpt":"","rawMarkdownBody":"# About\nMldraw is a new vector drawing tool that lets you play with machine learning! Users can mix cats, anime, Pikachu, handbags, imagined buildings and more.\nMldraw is a web app that uses a layered vector drawing system where each layer can be given a different machine learning model that translates user input. The user will give us a line drawing of edges, and our app's backend server renders the translation using whatever model is assigned to that layer.\nFurthermore, we've also made it easy to import custom ML models for researchers so that they can use our tool to experiment with their models!\nMade in collaboration with [Aman Tiwari](http://www.aman.work/)!\nI designed and built the UI using HTML/CSS/Typescript, and I also worked on the drawing and layer functionality, such as the\nvector drawing, drag and dropping objects, layer building, etc.\nMldraw’s interface is inspired by cute, techy/anti-techy retro aesthetics, such as the work of Sailor Mercury and the Bubblesort Zines.  We wanted it to be fun, novel, exciting and differentiated from the world of arxiv papers.\nMldraw was born out of seeing the potential of the body of research done using pix2pix to turn drawings into other images and the severe lack of a usable, “useful” and accessible tool to utilize this technology.\n\n# Videos\n\n<iframe class=\"iframe-560h\" src=\"https://player.vimeo.com/video/321893512\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n<iframe class=\"iframe-620h\" src=\"https://player.vimeo.com/video/378759111\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Still in progress\nWe have a great demo, but have plans to bug-test, add more exciting ML models and features, and deploy at mldraw.com. We're also still experimenting and making big changes to the UI / UX! More documentation coming soon.\n![edges2pikachu example](../images/mldraw/mldraw_beforeafter.jpg) *Left: User line drawing, Right: rendered version. Combined pikachu model and handbag model*\n\n# Implementation\nMldraw is implemented as a Typescript frontend using choo.js as a UI framework, with a Python registry server and a Python adapter library, along with a number of instantiations of the adapter library for specific models.\nThe frontend communicates with the registry server using socket.io, which then passes to the frontend a list of models and their URLs. The frontend then communicates directly to the models.  This enables us e.g. to host a registry server for Mldraw without having to pay the cost of hosting every model it supports.\nMldraw also supports models that run locally on the client (in the above video, the cat, Pikachu and bag models run locally, while the other models are hosted on remote servers).\nIn service of the above desire to make Mldraw extensible, we have made it easy to add a new model – all that is required is some Python interface to the model, and to define a function that takes in an image and returns an image. Our model adapter will handle the rest of it, including registering the model with the server hosting a Mldraw interface.\n\n# Progress Documentation\n\n<video autoplay loop muted poster=\"../images/mldraw/mldraw_sequence.jpg\">\n    <source src=\"../images/mldraw/mldraw_sequence.mp4\">\n</video>\n\n## CSS animations\n![render button interaction gif](../images/mldraw/button-interactions.gif)*Examples of small css animated interactions, Left: tooltip + pop animation on hover + lingering expansion until unhover, Middle: render button disappears when canvas in use, fades back in when done, Right: render button spins while rendering*\n\n\n\n![toolbar interaction gif](../images/mldraw/toolbar.gif)*toolbar animations: nod when usable, shake when unusable for that model*\n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/mldraw.md","__gatsby_resolved":{"frontmatter":{"path":"/ml-draw"}}},"previous":{"id":"5ad30901-276b-58db-8257-1bc53fbf6f26","children":[],"parent":"7c784d7b-7b13-5788-915d-56db6e2cdd29","internal":{"content":"\n# Primary Project\n\nMy intern project involved working in a team with a designer, Michelle Alvarez and a researcher, Dawn Shaikh, to create a prototype that we used in a user study to \ncompare various VF onboarding materials that educate users about the general benefits of VFs, the nature of VFs, and the registered/unregistered axes available in given typeface. \nThe study learnings will guide the design of both a material.io experience and the initial onboarding experience on Google Fonts.\n\n![google timeline](../images/google-material/goog-timeline.png)\n![images of primary project](../images/google-material/google1.gif)\n![images of primary project](../images/google-material/google2.gif)\n![images of primary project](../images/google-material/google3.gif)\n![images of primary project](../images/google-material/page1.gif)\n![images of primary project](../images/google-material/page5.gif)\n![images of primary project](../images/google-material/page7.gif)\n\n# Secondary Project\nFor my secondary project, I worked with Clayton Meador and Dave Crossland to deploy a microsite with interactive creative demos for variable fonts. \nI was given an incredible amount of agency for this project. The visual design and concepts are my own,\naided by the feedback by the many amazing designers on Material Design like Michelle Alvarez and Sharon Harris.\nThe site is currently live at [googlefonts.github.io/fluid](googlefonts.github.io/fluid).\n![images of secondary project](../images/google-material/second1.gif)\n![images of secondary project](../images/google-material/second2.gif)\n![images of secondary project](../images/google-material/second3.gif)\n![images of secondary project](../images/google-material/second4.gif)\n","type":"MarkdownRemark","contentDigest":"986ce01c276c980c22e2bd43a5a24ef9","counter":125,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"Google Material Design Internship","date":"Summer 2019","path":"/google-material","author":"Lorem Ipsum","coverImage":"../images/vf_google.jpg","coverVideo":"../assets/vf_google.mp4","links":"","excerpt":"Co-designed and engineered experimental sites piloting the design of future variable fonts experiences.","readMoreButton":"Learn more","order":0},"excerpt":"","rawMarkdownBody":"\n# Primary Project\n\nMy intern project involved working in a team with a designer, Michelle Alvarez and a researcher, Dawn Shaikh, to create a prototype that we used in a user study to \ncompare various VF onboarding materials that educate users about the general benefits of VFs, the nature of VFs, and the registered/unregistered axes available in given typeface. \nThe study learnings will guide the design of both a material.io experience and the initial onboarding experience on Google Fonts.\n\n![google timeline](../images/google-material/goog-timeline.png)\n![images of primary project](../images/google-material/google1.gif)\n![images of primary project](../images/google-material/google2.gif)\n![images of primary project](../images/google-material/google3.gif)\n![images of primary project](../images/google-material/page1.gif)\n![images of primary project](../images/google-material/page5.gif)\n![images of primary project](../images/google-material/page7.gif)\n\n# Secondary Project\nFor my secondary project, I worked with Clayton Meador and Dave Crossland to deploy a microsite with interactive creative demos for variable fonts. \nI was given an incredible amount of agency for this project. The visual design and concepts are my own,\naided by the feedback by the many amazing designers on Material Design like Michelle Alvarez and Sharon Harris.\nThe site is currently live at [googlefonts.github.io/fluid](googlefonts.github.io/fluid).\n![images of secondary project](../images/google-material/second1.gif)\n![images of secondary project](../images/google-material/second2.gif)\n![images of secondary project](../images/google-material/second3.gif)\n![images of secondary project](../images/google-material/second4.gif)\n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/Google_material.md","__gatsby_resolved":{"frontmatter":{"path":"/google-material"}}}}},"staticQueryHashes":["236058478","3128451518"]}