{"componentChunkName":"component---src-templates-page-js","path":"/ar-templates","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Augmented Reality Educational Templates","date":"Fall 2019","path":"/ar-templates","author":null,"excerpt":"Nine simple Unity demos for students in a creative computation course at Carnegie Mellon to use as a reference for their projects.","tags":["ARKit","ARCore","Unity","C#"],"coverImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABAABAv/EABQBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAWco0Tx5P//EABoQAAIDAQEAAAAAAAAAAAAAAAIDAAQTARH/2gAIAQEAAQUCSfSag/WbHF182BXFZT//xAAVEQEBAAAAAAAAAAAAAAAAAAAQIf/aAAgBAwEBPwGH/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAHBAAAgICAwAAAAAAAAAAAAAAAAEREhAhMUGh/9oACAEBAAY/AufR6g7LWksm5jH/xAAaEAADAQEBAQAAAAAAAAAAAAAAAREhYTGB/9oACAEBAAE/IZhtHYOKm7fTn9QXp4akGxzRayn/2gAMAwEAAgADAAAAEJzf/8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQMBAT8QY//EABcRAQADAAAAAAAAAAAAAAAAAAABESH/2gAIAQIBAT8Q1cP/xAAcEAEAAwACAwAAAAAAAAAAAAABABEhMVFBYZH/2gAIAQEAAT8QeMk7J47FqMe1DJNTzTB9CHaIT4MoNWgDH3GN4NSA71Lz/9k=","aspectRatio":1.6,"src":"/static/7662081d2705c09159d7ae9190a91b68/f9913/ar_phone_thumb.jpg","srcSet":"/static/7662081d2705c09159d7ae9190a91b68/f836f/ar_phone_thumb.jpg 200w,\n/static/7662081d2705c09159d7ae9190a91b68/2244e/ar_phone_thumb.jpg 400w,\n/static/7662081d2705c09159d7ae9190a91b68/f9913/ar_phone_thumb.jpg 750w","sizes":"(max-width: 750px) 100vw, 750px"}}},"coverVideo":{"publicURL":"/static/b2d0c2cd0c19e69b80a8008e35aacfaf/ar_phone.mp4"},"links":"iOS templates,https://github.com/CreativeInquiry/ARKit-Educational-Templates;Android templates,https://github.com/CreativeInquiry/ARCore-Educational-Templates"},"id":"7164d671-d395-51d5-905f-6da2cd89f39f","html":"<p class=\"para\">As a research assistant for the Studio for Creative Inquiry, I created simple educational Unity templates for both Android and iOS for students in a creative computation course at Carnegie Mellon to use as a reference for their AR projects. The demos covered the AR workflow for both platforms, face tracking, image anchors, computer vision, and more. </p>\n<h2>ARKit 2 Demos</h2>\n<p class=\"para\">Please view <a href=\"https://github.com/CreativeInquiry/ARKit-Educational-Templates\">this github repo</a> to view the work and more information.</p>\n<h2>ARCore Demos</h2>\n<p class=\"para\">Please view <a href=\"https://github.com/CreativeInquiry/ARCore-Educational-Templates/\">this github repo</a> to view the work and more information.</p>\n<video autoplay loop muted style=\"width: 35%;\" poster=\"../images/ar/ar_phone_thumb.jpg\">\n    <source src=\"../images/ar/ar_phone_2.mp4\">\n</video>","excerpt":"As a research assistant for the Studio for Creative Inquiry, I created simple educational Unity templates for both Android and iOS for…"}},"pageContext":{"type":"posts","next":{"id":"c1f1e56f-af6a-50bf-8fe4-872205c348c2","children":[],"parent":"07e3217e-6654-5ffa-8ef3-46a2d2f2a6c4","internal":{"content":"\n# Concept\n\nAs a research assistant in Oh!Lab at Carnegie Mellon, I collaborated with Joseph Seering and Michal Luria to design and program a \"Baby\" chatbot that was designed to interact with a community and grow up over time. Professors Jessica Hammer and Geoff Kaufmann advised on the project and final paper. \n\nIn this project, I designed the chatbot concept and interactions and was the sole programmer in charge of implementation as well. After deploying the study for two weeks, I handled the data analysis afterwards.\n\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/babybot/bacteria-baby.gif\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/babybot/toddler.gif\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/babybot/teenager-bot.gif\"/></div>\n</div>\n\nOur work builds on [a previous paper published by Joseph, Michal, Geoff Kaufman and Jessica Hammer](https://dl.acm.org/doi/10.1145/3290605.3300680). That paper notes that many current chatbots are designed for one-on-one interactions and argues that:  \n> \"chatbots' social roles and conversational capabilities beyond dyadic interactions have been underexplored, and that expansion into this design space could support richer social interactions in online communities and help address the longstanding challenges of maintaining, moderating, and growing these communities.\"\n\n# Process\n\n## Initial mockups\nFocusing in on the \"dependent\" chatbot suggested by the above paper, I designed many different chatbot concepts, iterated on these concepts and narrowed in onto one type of bot. \n<div id=\"illustrations\">\n    <img src=\"../images/babybot/babybot_initial_bot1.jpeg\" alt=\"concept of a chatbot where a chatbot is an adventurer\"/>\n    <img src=\"../images/babybot/babybot_initial_bot2.jpeg\" alt=\"concept of a chatbot where a chatbot communicates through visual motifs\"/>\n    <img src=\"../images/babybot/babybot_initial_bot3.jpeg\" alt=\"concept of a chatbot where a chatbot changes text over time\"/>\n    <img src=\"../images/babybot/babybot_initial_bot4.jpeg\" alt=\"concept of a chatbot where a chatbot is a mama's boy\"/>\n    <img src=\"../images/babybot/babybot_initial_bot5.png\" alt=\"concept of a chatbot where a chatbot is based on astrology\"/>\n</div>\n\n<div class=\"br\"></div>\n\n## Second iteration of refined mockups & user journey\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/babybot/advbot-card.jpg\"/></div>\n    <div class=\"flex-item\"><img class=\"flex-item\" src=\"../images/babybot/avdbot-journey.jpg\"/></div>\n</div>\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/babybot/love-card.jpg\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/babybot/love-journey.jpg\"/></div>\n</div>\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/babybot/vbot-card.jpg\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/babybot/visual-bot.jpg\"/></div>\n</div>\n\n*above: process illustrations of initial concepts*\n\n<div class=\"br\"></div>\n\n## Flow diagrams \nNext, I took the final concept of an adventuring chatbot to flesh out the possible interactions using this flow diagram.\n![flow diagram](../images/babybot/new-flow.jpg)\n\n<div class=\"br\"></div>\n\n## Development\nOnce we had designed all of the features of Babybot, I then programmed the bot. The chatbot used in this study was developed using Javascript and run locally using Node.js. Package management was handled through npm. We are using several javascript libraries for Twitch relaying, data saving and language generation; notably, we are using tmi.js (v1.4.2) to access Twitch’s Internet Relay Chat, and Rita.js (v1.3.89) to perform natural language processing on our text. \n\n![implementation](../images/babybot/babybot_flow.png)\n\n# For more information about the implementation, study and final insights, please see our paper.\n\n# [Link to paper](https://dl.acm.org/doi/abs/10.1145/3313831.3376708)\n\n## Abstract\n>\"While the majority of research in chatbot design has focused on creating chatbots that engage with users one-on-one, less work has focused on the design of conversational agents for online communities. In this paper we present results from a three week test of a social chatbot in an established online community. During this study, the chatbot \"grew up\" from \"birth\" through its teenage years, engaging with community members and \"learning\" vocabulary from their conversations. We discuss the design of this chatbot, how users' interactions with it evolved over the course of the study, and how it impacted the community as a whole. We discuss how we addressed challenges in developing a chatbot whose vocabulary could be shaped by users, and conclude with implications for the role of machine learning in social interactions in online communities and potential future directions for design of community-based chatbots.\"\n","type":"MarkdownRemark","contentDigest":"7234f0c2b88d04d0c7600a492ce8cd09","counter":154,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"Babybot","path":"/babybot","date":"Spring 2019 - Fall 2019","coverImage":"../images/babybot.jpg","coverVideo":"../assets/babybot.mp4","tags":["Twitch API","Node.js","UX Research","Chatbots"],"excerpt":"A Twitch chatbot built to bring an online community together. Our paper, It Takes a Village: Integrating an Adaptive Chatbot into an Online Gaming Community, was accepted into CHI 2020. ","links":"Link to paper,https://dl.acm.org/doi/abs/10.1145/3313831.3376708","readMoreButton":"Learn more","order":6,"shortTitle":"Babybot"},"excerpt":"","rawMarkdownBody":"\n# Concept\n\nAs a research assistant in Oh!Lab at Carnegie Mellon, I collaborated with Joseph Seering and Michal Luria to design and program a \"Baby\" chatbot that was designed to interact with a community and grow up over time. Professors Jessica Hammer and Geoff Kaufmann advised on the project and final paper. \n\nIn this project, I designed the chatbot concept and interactions and was the sole programmer in charge of implementation as well. After deploying the study for two weeks, I handled the data analysis afterwards.\n\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/babybot/bacteria-baby.gif\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/babybot/toddler.gif\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/babybot/teenager-bot.gif\"/></div>\n</div>\n\nOur work builds on [a previous paper published by Joseph, Michal, Geoff Kaufman and Jessica Hammer](https://dl.acm.org/doi/10.1145/3290605.3300680). That paper notes that many current chatbots are designed for one-on-one interactions and argues that:  \n> \"chatbots' social roles and conversational capabilities beyond dyadic interactions have been underexplored, and that expansion into this design space could support richer social interactions in online communities and help address the longstanding challenges of maintaining, moderating, and growing these communities.\"\n\n# Process\n\n## Initial mockups\nFocusing in on the \"dependent\" chatbot suggested by the above paper, I designed many different chatbot concepts, iterated on these concepts and narrowed in onto one type of bot. \n<div id=\"illustrations\">\n    <img src=\"../images/babybot/babybot_initial_bot1.jpeg\" alt=\"concept of a chatbot where a chatbot is an adventurer\"/>\n    <img src=\"../images/babybot/babybot_initial_bot2.jpeg\" alt=\"concept of a chatbot where a chatbot communicates through visual motifs\"/>\n    <img src=\"../images/babybot/babybot_initial_bot3.jpeg\" alt=\"concept of a chatbot where a chatbot changes text over time\"/>\n    <img src=\"../images/babybot/babybot_initial_bot4.jpeg\" alt=\"concept of a chatbot where a chatbot is a mama's boy\"/>\n    <img src=\"../images/babybot/babybot_initial_bot5.png\" alt=\"concept of a chatbot where a chatbot is based on astrology\"/>\n</div>\n\n<div class=\"br\"></div>\n\n## Second iteration of refined mockups & user journey\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/babybot/advbot-card.jpg\"/></div>\n    <div class=\"flex-item\"><img class=\"flex-item\" src=\"../images/babybot/avdbot-journey.jpg\"/></div>\n</div>\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/babybot/love-card.jpg\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/babybot/love-journey.jpg\"/></div>\n</div>\n<div class=\"fill-row\">\n    <div class=\"flex-item-start\"><img src=\"../images/babybot/vbot-card.jpg\"/></div>\n    <div class=\"flex-item\"><img src=\"../images/babybot/visual-bot.jpg\"/></div>\n</div>\n\n*above: process illustrations of initial concepts*\n\n<div class=\"br\"></div>\n\n## Flow diagrams \nNext, I took the final concept of an adventuring chatbot to flesh out the possible interactions using this flow diagram.\n![flow diagram](../images/babybot/new-flow.jpg)\n\n<div class=\"br\"></div>\n\n## Development\nOnce we had designed all of the features of Babybot, I then programmed the bot. The chatbot used in this study was developed using Javascript and run locally using Node.js. Package management was handled through npm. We are using several javascript libraries for Twitch relaying, data saving and language generation; notably, we are using tmi.js (v1.4.2) to access Twitch’s Internet Relay Chat, and Rita.js (v1.3.89) to perform natural language processing on our text. \n\n![implementation](../images/babybot/babybot_flow.png)\n\n# For more information about the implementation, study and final insights, please see our paper.\n\n# [Link to paper](https://dl.acm.org/doi/abs/10.1145/3313831.3376708)\n\n## Abstract\n>\"While the majority of research in chatbot design has focused on creating chatbots that engage with users one-on-one, less work has focused on the design of conversational agents for online communities. In this paper we present results from a three week test of a social chatbot in an established online community. During this study, the chatbot \"grew up\" from \"birth\" through its teenage years, engaging with community members and \"learning\" vocabulary from their conversations. We discuss the design of this chatbot, how users' interactions with it evolved over the course of the study, and how it impacted the community as a whole. We discuss how we addressed challenges in developing a chatbot whose vocabulary could be shaped by users, and conclude with implications for the role of machine learning in social interactions in online communities and potential future directions for design of community-based chatbots.\"\n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/babybot.md","__gatsby_resolved":{"frontmatter":{"path":"/babybot"}}},"previous":{"id":"b137c10c-c88c-5886-81f1-c54d8278992c","children":[],"parent":"6eb1fc43-2c27-5914-8631-5c5b1607e9b3","internal":{"content":"\nA collaboration with Alice Fang and Sebastian Carpenter.\n\nZoöid is an exploration of light and color as a means for communication in a starless world. Inspired by deep-sea creatures that use light to communicate and coordinate, the looks in the line create an intricate lightshow on stage.\n\nThis project was funded with support by Carnegie Mellon’s Undergraduate Research Office, and the Frank-Ratchye Studio for Creative Inquiry. \n\n# Show video\n\n<iframe class=\"video-iframe\" src=\"https://player.vimeo.com/video/423457555\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Show images\n![zooid cover image](../images/zooid/zooid_1.JPG)\n![zooid cover image](../images/zooid/zooid_2.JPG)\n![zooid cover image](../images/zooid/zooid_3.JPG)\n![zooid cover image](../images/zooid/zooid_4.JPG)\n![zooid cover image](../images/zooid/zooid_end.JPG)\n\n# Concept\n\nInspired by Pyrosomes–deep sea colonies of small animals that use light to communicate and coordinate their actions–we are interested in exploring the ways light-based communication is used to synchronize actions; to achieve this, we want to create wearables that experiment with giving humans the same capabilities. Thus, our medium of dynamic light-based wearables is critical for our exploration, and our use of physical computation, including the development of a data-transfer protocol, is critical to achieving the level of fidelity necessary for our project.\n\n![zooid cover image](../images/zooid/moodboard.jpg)\n\nWe are hugely inspired by prior work by researchers who create fascinating cutting-edge wearables with real-life implications, such as Electrodermis by Morphing Matter Lab and Digits Nail Technologies. Visually, we are inspired by organic forms, like the work of Iris van Herpen.  However, although there are many instances of technologists who create specifically LED wearables, we have struggled to find many compelling examples of pieces that can work collectively. We are attempting to fill a gap in trends that we have observed— although physical computing is becoming more accessible, the use of LED lights in wearables remains at a one-dimensional, ornamental level, without meaningful integration. We want to explore LED wearables from a different angle, not using the lights as the only focus but rather as a medium in which we express an underlying networked system.\n\nWith clothing as part of a networked system that works collectively in communication with each other, instead of independent and isolated bodies, possibilities open up to the implications of ubiquitous technology, and the role it can play in future clothing design in the realm of performance, streetwear, disabilities resources and safety methods. For instance, we believe our system could potentially be adapted to benefit large groups working in dark or noisy environments, such as construction workers or firefighters: lights can be used to coordinate roles, and are capable of communicating across distance.\n\n\n# Process images\n\nFor the process of garment construction, base garments will be constructed with traditional sewing techniques, using tulle, spandex, cotton, and chiffon. The structural parts that house electronics will be created from 3D-printed flexible plastic that will allow us to print flat pieces that can conform to the model’s body. Our outfits use Arduinos and XBees, which allow each outfit to wirelessly communicate with a computer, which processes incoming data, such as music. Based on that data, color schemes and intensities are sent to XBee on each outfit, which receives the color scheme and intensity. We will develop our own protocol for data transfer in order to avoid the computer receiving multiple signals that interfere with each other. Colors change according to instructions received by the XBee, and the outfit will be lit with Adafruit NeoPixel LEDs. The lights will be dispersed through tulle fabric, fiber-optics, or acrylic plastic. \n\n![zooid cover image](../images/zooid/zooid-process3.jpg)\n![zooid cover image](../images/zooid/zooid-process4.jpg)\n![zooid cover image](../images/zooid/zooid-process5.jpg)\n![zooid cover image](../images/zooid/zooid-process6.jpg)\n\n","type":"MarkdownRemark","contentDigest":"779361137ab1cb4e22549ca9e22419f5","counter":168,"owner":"gatsby-transformer-remark"},"frontmatter":{"title":"Zooïd Lunar Gala","date":"Fall 2019 - Spring 2020","path":"/zooid","author":"Lorem Ipsum","coverImage":"../images/zooid_thumb.jpg","coverVideo":"../assets/zooid_shadow.mp4","links":"","tags":["Physical computing","Arduino","Soft fabrication"],"excerpt":"A line of eight light-art wearables, made for one of Pittsburgh's largest fashion shows.","readMoreButton":"See documentation","order":4,"shortTitle":"Zooid"},"excerpt":"","rawMarkdownBody":"\nA collaboration with Alice Fang and Sebastian Carpenter.\n\nZoöid is an exploration of light and color as a means for communication in a starless world. Inspired by deep-sea creatures that use light to communicate and coordinate, the looks in the line create an intricate lightshow on stage.\n\nThis project was funded with support by Carnegie Mellon’s Undergraduate Research Office, and the Frank-Ratchye Studio for Creative Inquiry. \n\n# Show video\n\n<iframe class=\"video-iframe\" src=\"https://player.vimeo.com/video/423457555\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n\n# Show images\n![zooid cover image](../images/zooid/zooid_1.JPG)\n![zooid cover image](../images/zooid/zooid_2.JPG)\n![zooid cover image](../images/zooid/zooid_3.JPG)\n![zooid cover image](../images/zooid/zooid_4.JPG)\n![zooid cover image](../images/zooid/zooid_end.JPG)\n\n# Concept\n\nInspired by Pyrosomes–deep sea colonies of small animals that use light to communicate and coordinate their actions–we are interested in exploring the ways light-based communication is used to synchronize actions; to achieve this, we want to create wearables that experiment with giving humans the same capabilities. Thus, our medium of dynamic light-based wearables is critical for our exploration, and our use of physical computation, including the development of a data-transfer protocol, is critical to achieving the level of fidelity necessary for our project.\n\n![zooid cover image](../images/zooid/moodboard.jpg)\n\nWe are hugely inspired by prior work by researchers who create fascinating cutting-edge wearables with real-life implications, such as Electrodermis by Morphing Matter Lab and Digits Nail Technologies. Visually, we are inspired by organic forms, like the work of Iris van Herpen.  However, although there are many instances of technologists who create specifically LED wearables, we have struggled to find many compelling examples of pieces that can work collectively. We are attempting to fill a gap in trends that we have observed— although physical computing is becoming more accessible, the use of LED lights in wearables remains at a one-dimensional, ornamental level, without meaningful integration. We want to explore LED wearables from a different angle, not using the lights as the only focus but rather as a medium in which we express an underlying networked system.\n\nWith clothing as part of a networked system that works collectively in communication with each other, instead of independent and isolated bodies, possibilities open up to the implications of ubiquitous technology, and the role it can play in future clothing design in the realm of performance, streetwear, disabilities resources and safety methods. For instance, we believe our system could potentially be adapted to benefit large groups working in dark or noisy environments, such as construction workers or firefighters: lights can be used to coordinate roles, and are capable of communicating across distance.\n\n\n# Process images\n\nFor the process of garment construction, base garments will be constructed with traditional sewing techniques, using tulle, spandex, cotton, and chiffon. The structural parts that house electronics will be created from 3D-printed flexible plastic that will allow us to print flat pieces that can conform to the model’s body. Our outfits use Arduinos and XBees, which allow each outfit to wirelessly communicate with a computer, which processes incoming data, such as music. Based on that data, color schemes and intensities are sent to XBee on each outfit, which receives the color scheme and intensity. We will develop our own protocol for data transfer in order to avoid the computer receiving multiple signals that interfere with each other. Colors change according to instructions received by the XBee, and the outfit will be lit with Adafruit NeoPixel LEDs. The lights will be dispersed through tulle fabric, fiber-optics, or acrylic plastic. \n\n![zooid cover image](../images/zooid/zooid-process3.jpg)\n![zooid cover image](../images/zooid/zooid-process4.jpg)\n![zooid cover image](../images/zooid/zooid-process5.jpg)\n![zooid cover image](../images/zooid/zooid-process6.jpg)\n\n","fileAbsolutePath":"/Users/crabbage/Documents/newwebdev/src/posts/zooid.md","__gatsby_resolved":{"frontmatter":{"path":"/zooid"}}}}},"staticQueryHashes":["236058478","3128451518"]}